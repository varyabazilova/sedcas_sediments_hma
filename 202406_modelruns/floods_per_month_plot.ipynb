{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a205723-2b45-48ff-ad15-c1c0859736bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varyabazilova/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd9c6d9-67c0-4f66-985c-971c30205723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the % of the dfs and dfspot for each scenario \n",
    "\n",
    "def prepare_dfcount_for_plot(dfcount):\n",
    "    melted = pd.melt(dfcount, id_vars=['D', 'D_year', 'D_month'], var_name='elevation', value_name='dfs_count')\n",
    "    melted['elevation'] = melted['elevation'].str.split('.').str[0].astype(float)\n",
    "    melted['elevation_bin'] = melted.apply(functions.bin_elevation500, axis=1)\n",
    "    # melted = melted.sort_values('elevation_bin')\n",
    "    # melted = melted.dropna(subset=['dfs_count'])\n",
    "    return melted \n",
    "\n",
    "\n",
    "def cut_dates(df):\n",
    "    df['D'] = pd.to_datetime(df['D'])\n",
    "    filtered_df = df[df['D'] > '1990-08-30']\n",
    "    filtered_df = filtered_df[filtered_df['D'] > '1990-07-31']\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882f8784-6ab6-4f67-8a6a-0fbfe958d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TL case, so potential df (dsfspot)\n",
    "\n",
    "freq = 'monthly'\n",
    "column = 'Qdftl'\n",
    "location = 'langtang'\n",
    "\n",
    "\n",
    "flood_folder =  f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/dfspot_count/{location}/'\n",
    "\n",
    "dfspot1 = cut_dates(pd.read_csv(flood_folder + f'{location}_{freq}_{column}_count_landcover1.csv', index_col = 0).fillna(0))\n",
    "dfspot2 = cut_dates(pd.read_csv(flood_folder + f'{location}_{freq}_{column}_count_landcover2.csv', index_col = 0).fillna(0))\n",
    "dfspot3 = cut_dates(pd.read_csv(flood_folder + f'{location}_{freq}_{column}_count_landcover3.csv', index_col = 0).fillna(0))\n",
    "dfspot4 = cut_dates(pd.read_csv(flood_folder + f'{location}_{freq}_{column}_count_landcover4.csv', index_col = 0).fillna(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da601fe-1db8-4e55-bb26-d3c1b1dc9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspot1_melted = prepare_dfcount_for_plot(dfspot1)\n",
    "dfspot2_melted = prepare_dfcount_for_plot(dfspot2)\n",
    "dfspot3_melted = prepare_dfcount_for_plot(dfspot3)\n",
    "dfspot4_melted = prepare_dfcount_for_plot(dfspot4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999f1bf9-5c49-46aa-ac4f-f1ac7935e7a0",
   "metadata": {},
   "source": [
    "# daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74123027-3395-47b9-b672-30d0dd495e34",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "daily20_lc1 = cut_dates(pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_20percent/{location}_{freq}_df_count_20percent_landcover1.csv', index_col = 0).fillna(0))#[:372]\n",
    "daily20_lc2 = cut_dates(pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_20percent/{location}_{freq}_df_count_20percent_landcover2.csv', index_col = 0).fillna(0))#[:372]\n",
    "daily20_lc3 = cut_dates(pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_20percent/{location}_{freq}_df_count_20percent_landcover3.csv', index_col = 0).fillna(0))#[:-1]#[:372]\n",
    "daily20_lc4 = cut_dates(pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_20percent/{location}_{freq}_df_count_20percent_landcover4.csv', index_col = 0).fillna(0))#[:372]\n",
    "\n",
    "daily30_lc1 = cut_dates(pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_30percent/{location}_{freq}_df_count_30percent_landcover1.csv', index_col = 0).fillna(0))#[:372]\n",
    "daily30_lc2 = cut_dates(pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_30percent/{location}_{freq}_df_count_30percent_landcover2.csv', index_col = 0).fillna(0))#[:372]\n",
    "daily30_lc3 = cut_dates(pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_30percent/{location}_{freq}_df_count_30percent_landcover3.csv', index_col = 0).fillna(0))#[:372]\n",
    "daily30_lc4 = cut_dates(pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_30percent/{location}_{freq}_df_count_30percent_landcover4.csv', index_col = 0).fillna(0))#[:372]\n",
    "\n",
    "daily40_lc1 = cut_dates(pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_40percent/{location}_{freq}_df_count_40percent_landcover1.csv', index_col = 0).fillna(0))#[:372]\n",
    "daily40_lc2 = cut_dates(pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_40percent/{location}_{freq}_df_count_40percent_landcover2.csv', index_col = 0).fillna(0))#[:372]\n",
    "daily40_lc3 = cut_dates(pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_40percent/{location}_{freq}_df_count_40percent_landcover3.csv', index_col = 0).fillna(0))#[:372]\n",
    "daily40_lc4 = cut_dates(pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_40percent/{location}_{freq}_df_count_40percent_landcover4.csv', index_col = 0).fillna(0))#[:372]\n",
    "\n",
    "\n",
    "daily20_lc1_melted = prepare_dfcount_for_plot(daily20_lc1).rename(columns={'dfs_count': 'dfs_count_daily20'})\n",
    "daily20_lc2_melted = prepare_dfcount_for_plot(daily20_lc2).rename(columns={'dfs_count': 'dfs_count_daily20'})\n",
    "daily20_lc3_melted = prepare_dfcount_for_plot(daily20_lc3).rename(columns={'dfs_count': 'dfs_count_daily20'})\n",
    "daily20_lc4_melted = prepare_dfcount_for_plot(daily20_lc4).rename(columns={'dfs_count': 'dfs_count_daily20'})\n",
    "\n",
    "daily30_lc1_melted = prepare_dfcount_for_plot(daily30_lc1).rename(columns={'dfs_count': 'dfs_count_daily30'})\n",
    "daily30_lc2_melted = prepare_dfcount_for_plot(daily30_lc2).rename(columns={'dfs_count': 'dfs_count_daily30'})\n",
    "daily30_lc3_melted = prepare_dfcount_for_plot(daily30_lc3).rename(columns={'dfs_count': 'dfs_count_daily30'})\n",
    "daily30_lc4_melted = prepare_dfcount_for_plot(daily30_lc4).rename(columns={'dfs_count': 'dfs_count_daily30'})\n",
    "\n",
    "daily40_lc1_melted = prepare_dfcount_for_plot(daily40_lc1).rename(columns={'dfs_count': 'dfs_count_daily40'})\n",
    "daily40_lc2_melted = prepare_dfcount_for_plot(daily40_lc2).rename(columns={'dfs_count': 'dfs_count_daily40'})\n",
    "daily40_lc3_melted = prepare_dfcount_for_plot(daily40_lc3).rename(columns={'dfs_count': 'dfs_count_daily40'})\n",
    "daily40_lc4_melted = prepare_dfcount_for_plot(daily40_lc4).rename(columns={'dfs_count': 'dfs_count_daily40'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fbd0197-f396-4060-80ea-213788b168a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 4 dfs depending ont he scenatio \n",
    "merge_on = ['D', 'D_year', 'D_month', 'elevation', 'elevation_bin']\n",
    "\n",
    "# ------- landcover1\n",
    "dfs = [daily20_lc1_melted, daily30_lc1_melted, daily40_lc1_melted]\n",
    "\n",
    "lc1_daily = dfs[0][merge_on + ['dfs_count_daily20']]\n",
    "# Iterate over the remaining dataframes (daily30, daily40) and merge them\n",
    "for i, (df, label) in enumerate(zip(dfs[1:], [30, 40]), start=2):\n",
    "    unique_col = f'dfs_count_daily{label}'  # Adjust unique column name\n",
    "    lc1_daily = pd.merge(lc1_daily, df[merge_on + [unique_col]], on=merge_on)\n",
    "\n",
    "# add dfspot    \n",
    "lc1_daily['dfspot_count'] = dfspot1_melted.dfs_count\n",
    "\n",
    "# ------- landcover2\n",
    "dfs = [daily20_lc2_melted, daily30_lc2_melted, daily40_lc2_melted]\n",
    "\n",
    "lc2_daily = dfs[0][merge_on + ['dfs_count_daily20']]\n",
    "# Iterate over the remaining dataframes (daily30, daily40) and merge them\n",
    "for i, (df, label) in enumerate(zip(dfs[1:], [30, 40]), start=2):\n",
    "    unique_col = f'dfs_count_daily{label}'  # Adjust unique column name\n",
    "    lc2_daily = pd.merge(lc2_daily, df[merge_on + [unique_col]], on=merge_on)\n",
    "\n",
    "lc2_daily['dfspot_count'] = dfspot2_melted.dfs_count\n",
    "\n",
    "# ------- landcover3\n",
    "dfs = [daily20_lc3_melted, daily30_lc3_melted, daily40_lc3_melted]\n",
    "\n",
    "lc3_daily = dfs[0][merge_on + ['dfs_count_daily20']]\n",
    "# Iterate over the remaining dataframes (daily30, daily40) and merge them\n",
    "for i, (df, label) in enumerate(zip(dfs[1:], [30, 40]), start=2):\n",
    "    unique_col = f'dfs_count_daily{label}'  # Adjust unique column name\n",
    "    lc3_daily = pd.merge(lc3_daily, df[merge_on + [unique_col]], on=merge_on)\n",
    "\n",
    "lc3_daily['dfspot_count'] = dfspot3_melted.dfs_count\n",
    "\n",
    "# ------- landcover4\n",
    "dfs = [daily20_lc4_melted, daily30_lc4_melted, daily40_lc4_melted]\n",
    "\n",
    "lc4_daily = dfs[0][merge_on + ['dfs_count_daily20']]\n",
    "# Iterate over the remaining dataframes (daily30, daily40) and merge them\n",
    "for i, (df, label) in enumerate(zip(dfs[1:], [30, 40]), start=2):\n",
    "    unique_col = f'dfs_count_daily{label}'  # Adjust unique column name\n",
    "    lc4_daily = pd.merge(lc4_daily, df[merge_on + [unique_col]], on=merge_on)\n",
    "\n",
    "lc4_daily['dfspot_count'] = dfspot4_melted.dfs_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccaa0f75-756d-4963-91c0-31578790b4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D</th>\n",
       "      <th>D_year</th>\n",
       "      <th>D_month</th>\n",
       "      <th>elevation</th>\n",
       "      <th>elevation_bin</th>\n",
       "      <th>dfs_count_daily20</th>\n",
       "      <th>dfs_count_daily30</th>\n",
       "      <th>dfs_count_daily40</th>\n",
       "      <th>dfspot_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-08-31</td>\n",
       "      <td>1990</td>\n",
       "      <td>8</td>\n",
       "      <td>4485.0</td>\n",
       "      <td>4000 - 4500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-09-30</td>\n",
       "      <td>1990</td>\n",
       "      <td>9</td>\n",
       "      <td>4485.0</td>\n",
       "      <td>4000 - 4500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-10-31</td>\n",
       "      <td>1990</td>\n",
       "      <td>10</td>\n",
       "      <td>4485.0</td>\n",
       "      <td>4000 - 4500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-11-30</td>\n",
       "      <td>1990</td>\n",
       "      <td>11</td>\n",
       "      <td>4485.0</td>\n",
       "      <td>4000 - 4500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-12-31</td>\n",
       "      <td>1990</td>\n",
       "      <td>12</td>\n",
       "      <td>4485.0</td>\n",
       "      <td>4000 - 4500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19287</th>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>5936.0</td>\n",
       "      <td>5500 - 6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19288</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>5936.0</td>\n",
       "      <td>5500 - 6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19289</th>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>5936.0</td>\n",
       "      <td>5500 - 6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19290</th>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>5936.0</td>\n",
       "      <td>5500 - 6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19291</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>5936.0</td>\n",
       "      <td>5500 - 6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19292 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               D  D_year  D_month  elevation elevation_bin  dfs_count_daily20  \\\n",
       "0     1990-08-31    1990        8     4485.0   4000 - 4500                0.0   \n",
       "1     1990-09-30    1990        9     4485.0   4000 - 4500                0.0   \n",
       "2     1990-10-31    1990       10     4485.0   4000 - 4500                0.0   \n",
       "3     1990-11-30    1990       11     4485.0   4000 - 4500                0.0   \n",
       "4     1990-12-31    1990       12     4485.0   4000 - 4500                0.0   \n",
       "...          ...     ...      ...        ...           ...                ...   \n",
       "19287 2021-02-28    2021        2     5936.0   5500 - 6000                0.0   \n",
       "19288 2021-03-31    2021        3     5936.0   5500 - 6000                0.0   \n",
       "19289 2021-04-30    2021        4     5936.0   5500 - 6000                0.0   \n",
       "19290 2021-05-31    2021        5     5936.0   5500 - 6000                0.0   \n",
       "19291 2021-06-30    2021        6     5936.0   5500 - 6000                0.0   \n",
       "\n",
       "       dfs_count_daily30  dfs_count_daily40  dfspot_count  \n",
       "0                    0.0                2.0           2.0  \n",
       "1                    0.0                0.0           1.0  \n",
       "2                    0.0                0.0           0.0  \n",
       "3                    0.0                0.0           0.0  \n",
       "4                    0.0                0.0           0.0  \n",
       "...                  ...                ...           ...  \n",
       "19287                0.0                0.0           0.0  \n",
       "19288                0.0                0.0           0.0  \n",
       "19289                0.0                0.0           0.0  \n",
       "19290                0.0                0.0           0.0  \n",
       "19291                0.0                0.0           0.0  \n",
       "\n",
       "[19292 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad17822-7f7e-4b39-ab2d-6c6b65e2d2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8f2e7-b40a-43d5-823b-efa70d9350a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1e6ff-4f37-4120-98b3-e9cd0e609009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba86741-cdf4-4ea7-a98b-bed3f10eba3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea253c67-c436-4dba-89f8-cc78f873d029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3cae74-d568-47ef-aef6-37993ad0402d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d017e7c8-326c-48f2-b649-1fafd857838e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150e6bd-b15a-4c30-8f02-98623779df4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81236c4b-6c4c-4f68-8b6b-623ee8c6fa7a",
   "metadata": {},
   "source": [
    "# once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95060edb-f6db-418c-9239-a1feba58609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "once20_lc1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_once/output_20percent/{location}_{freq}_df_count_20percent_landcover1.csv', index_col = 0).fillna(0)#[:372]\n",
    "once20_lc2 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_once/output_20percent/{location}_{freq}_df_count_20percent_landcover2.csv', index_col = 0).fillna(0)#[:372]\n",
    "once20_lc3 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_once/output_20percent/{location}_{freq}_df_count_20percent_landcover3.csv', index_col = 0).fillna(0)[:-1]#[:372]\n",
    "once20_lc4 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_once/output_20percent/{location}_{freq}_df_count_20percent_landcover4.csv', index_col = 0).fillna(0)#[:372]\n",
    "\n",
    "once30_lc1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_once/output_30percent/{location}_{freq}_df_count_30percent_landcover1.csv', index_col = 0).fillna(0)#[:372]\n",
    "once30_lc2 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_once/output_30percent/{location}_{freq}_df_count_30percent_landcover2.csv', index_col = 0).fillna(0)#[:372]\n",
    "once30_lc3 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_once/output_30percent/{location}_{freq}_df_count_30percent_landcover3.csv', index_col = 0).fillna(0)#[:372]\n",
    "once30_lc4 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_once/output_30percent/{location}_{freq}_df_count_30percent_landcover4.csv', index_col = 0).fillna(0)#[:372]\n",
    "\n",
    "once40_lc1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_once/output_40percent/{location}_{freq}_df_count_40percent_landcover1.csv', index_col = 0).fillna(0)#[:372]\n",
    "once40_lc2 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_once/output_40percent/{location}_{freq}_df_count_40percent_landcover2.csv', index_col = 0).fillna(0)#[:372]\n",
    "once40_lc3 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_once/output_40percent/{location}_{freq}_df_count_40percent_landcover3.csv', index_col = 0).fillna(0)#[:372]\n",
    "once40_lc4 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_once/output_40percent/{location}_{freq}_df_count_40percent_landcover4.csv', index_col = 0).fillna(0)#[:372]\n",
    "\n",
    "\n",
    "once20_lc1_melted = prepare_dfcount_for_plot(once20_lc1).rename(columns={'dfs_count': 'dfs_count_once20'})\n",
    "once20_lc2_melted = prepare_dfcount_for_plot(once20_lc2).rename(columns={'dfs_count': 'dfs_count_once20'})\n",
    "once20_lc3_melted = prepare_dfcount_for_plot(once20_lc3).rename(columns={'dfs_count': 'dfs_count_once20'})\n",
    "once20_lc4_melted = prepare_dfcount_for_plot(once20_lc4).rename(columns={'dfs_count': 'dfs_count_once20'})\n",
    "\n",
    "once30_lc1_melted = prepare_dfcount_for_plot(once30_lc1).rename(columns={'dfs_count': 'dfs_count_once30'})\n",
    "once30_lc2_melted = prepare_dfcount_for_plot(once30_lc2).rename(columns={'dfs_count': 'dfs_count_once30'})\n",
    "once30_lc3_melted = prepare_dfcount_for_plot(once30_lc3).rename(columns={'dfs_count': 'dfs_count_once30'})\n",
    "once30_lc4_melted = prepare_dfcount_for_plot(once30_lc4).rename(columns={'dfs_count': 'dfs_count_once30'})\n",
    "\n",
    "once40_lc1_melted = prepare_dfcount_for_plot(once40_lc1).rename(columns={'dfs_count': 'dfs_count_once40'})\n",
    "once40_lc2_melted = prepare_dfcount_for_plot(once40_lc2).rename(columns={'dfs_count': 'dfs_count_once40'})\n",
    "once40_lc3_melted = prepare_dfcount_for_plot(once40_lc3).rename(columns={'dfs_count': 'dfs_count_once40'})\n",
    "once40_lc4_melted = prepare_dfcount_for_plot(once40_lc4).rename(columns={'dfs_count': 'dfs_count_once40'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1028fa25-5556-4bdb-afb3-948991a9f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 4 dfs depending ont he scenatio \n",
    "merge_on = ['D', 'D_year', 'D_month', 'elevation', 'elevation_bin']\n",
    "\n",
    "# ------- landcover1\n",
    "dfs = [once20_lc1_melted, once30_lc1_melted, once40_lc1_melted]\n",
    "\n",
    "lc1_once = dfs[0][merge_on + ['dfs_count_once20']]\n",
    "# Iterate over the remaining dataframes (daily30, daily40) and merge them\n",
    "for i, (df, label) in enumerate(zip(dfs[1:], [30, 40]), start=2):\n",
    "    unique_col = f'dfs_count_once{label}'  # Adjust unique column name\n",
    "    lc1_once = pd.merge(lc1_once, df[merge_on + [unique_col]], on=merge_on)\n",
    "lc1_once['dfspot_count'] = dfspot1_melted.dfs_count\n",
    "\n",
    "# ------- landcover2\n",
    "dfs = [once20_lc2_melted, once30_lc2_melted, once40_lc2_melted]\n",
    "\n",
    "lc2_once = dfs[0][merge_on + ['dfs_count_once20']]\n",
    "# Iterate over the remaining dataframes (daily30, daily40) and merge them\n",
    "for i, (df, label) in enumerate(zip(dfs[1:], [30, 40]), start=2):\n",
    "    unique_col = f'dfs_count_once{label}'  # Adjust unique column name\n",
    "    lc2_once = pd.merge(lc2_once, df[merge_on + [unique_col]], on=merge_on)\n",
    "lc2_once['dfspot_count'] = dfspot2_melted.dfs_count\n",
    "\n",
    "# ------- landcover3\n",
    "dfs = [once20_lc3_melted, once30_lc3_melted, once40_lc3_melted]\n",
    "\n",
    "lc3_once = dfs[0][merge_on + ['dfs_count_once20']]\n",
    "# Iterate over the remaining dataframes (daily30, daily40) and merge them\n",
    "for i, (df, label) in enumerate(zip(dfs[1:], [30, 40]), start=2):\n",
    "    unique_col = f'dfs_count_once{label}'  # Adjust unique column name\n",
    "    lc3_once = pd.merge(lc3_once, df[merge_on + [unique_col]], on=merge_on)\n",
    "lc3_once['dfspot_count'] = dfspot3_melted.dfs_count\n",
    "\n",
    "\n",
    "# ------- landcover4\n",
    "dfs = [once20_lc4_melted, once30_lc4_melted, once40_lc4_melted]\n",
    "\n",
    "lc4_once = dfs[0][merge_on + ['dfs_count_once20']]\n",
    "# Iterate over the remaining dataframes (daily30, daily40) and merge them\n",
    "for i, (df, label) in enumerate(zip(dfs[1:], [30, 40]), start=2):\n",
    "    unique_col = f'dfs_count_once{label}'  # Adjust unique column name\n",
    "    lc4_once = pd.merge(lc4_once, df[merge_on + [unique_col]], on=merge_on)\n",
    "lc4_once['dfspot_count'] = dfspot4_melted.dfs_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5289e-7ec2-44db-9b42-ad5a8d340622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5f78d-5e56-4877-a5ab-0925488ba257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
