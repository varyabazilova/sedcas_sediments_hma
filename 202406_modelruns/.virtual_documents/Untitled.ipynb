import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import functions





def prepare_dfcount_for_plot(dfcount):
    melted = pd.melt(dfcount, id_vars=['date_id', 'year', 'month'], var_name='elevation', value_name='dfs_count')
    # melted = pd.melt(dfcount, id_vars=['date_id'], var_name='elevation', value_name='dfs_count')
    melted['elevation'] = melted['elevation'].str.split('.').str[0].astype(float)
    melted['elevation_bin'] = melted.apply(functions.bin_elevation500, axis=1)
    # create unique id for elevation and month for merging
    melted['elevation'] = melted['elevation'].astype(int)
    melted['id'] = melted['elevation'].astype(str) + "_" + melted['year'].astype(str)+ "_" + melted['month'].astype(str)
    
    # melted = melted.sort_values('elevation_bin')
    # melted = melted.dropna(subset=['dfs_count'])
    return melted 


# def prepare_dfcount_for_plot(dfcount):
#     melted = pd.melt(dfcount, id_vars=['date_id', 'year', 'month'], var_name='elevation', value_name='dfs_count')
#     melted['elevation'] = melted['elevation'].str.split('.').str[0].astype(float)
#     melted['elevation_bin'] = melted.apply(functions.bin_elevation500, axis=1)
#     # create unique id for elevation and month for merging
#     melted['elevation'] = melted['elevation'].astype(int)
#     # melted['id'] = melted['elevation'].astype(str) + "_" + melted['D_month'].astype(str)
    
#     # melted = melted.sort_values('elevation_bin')
#     # melted = melted.dropna(subset=['dfs_count'])
#     return melted 



freq = 'monthly'
column = 'dfspot'
location = 'langtang'


flood_folder =  f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/dfspot_count/{location}2/'

dfspot1 = pd.read_csv(flood_folder + f'{location}_{freq}_{column}_count_landcover1.csv', index_col = 0).fillna(0)
dfspot2 = pd.read_csv(flood_folder + f'{location}_{freq}_{column}_count_landcover2.csv', index_col = 0).fillna(0)
dfspot3 = pd.read_csv(flood_folder + f'{location}_{freq}_{column}_count_landcover3.csv', index_col = 0).fillna(0)
dfspot4 = pd.read_csv(flood_folder + f'{location}_{freq}_{column}_count_landcover4.csv', index_col = 0).fillna(0)

dfspot1_melted = prepare_dfcount_for_plot(dfspot1)
dfspot2_melted = prepare_dfcount_for_plot(dfspot2)
dfspot3_melted = prepare_dfcount_for_plot(dfspot3)
dfspot4_melted = prepare_dfcount_for_plot(dfspot4)






daily20_lc1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_20percent/{location}_{freq}_dfs_count_20percent_landcover1.csv', index_col = 0).fillna(0)
daily20_lc2 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_20percent/{location}_{freq}_dfs_count_20percent_landcover2.csv', index_col = 0).fillna(0)
daily20_lc3 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_20percent/{location}_{freq}_dfs_count_20percent_landcover3.csv', index_col = 0).fillna(0)
daily20_lc4 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_20percent/{location}_{freq}_dfs_count_20percent_landcover4.csv', index_col = 0).fillna(0)

daily30_lc1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_30percent/{location}_{freq}_dfs_count_30percent_landcover1.csv', index_col = 0).fillna(0)
daily30_lc2 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_30percent/{location}_{freq}_dfs_count_30percent_landcover2.csv', index_col = 0).fillna(0)
daily30_lc3 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_30percent/{location}_{freq}_dfs_count_30percent_landcover3.csv', index_col = 0).fillna(0)
daily30_lc4 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_30percent/{location}_{freq}_dfs_count_30percent_landcover4.csv', index_col = 0).fillna(0)

daily40_lc1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_40percent/{location}_{freq}_dfs_count_40percent_landcover1.csv', index_col = 0).fillna(0)
daily40_lc2 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_40percent/{location}_{freq}_dfs_count_40percent_landcover2.csv', index_col = 0).fillna(0)
daily40_lc3 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_40percent/{location}_{freq}_dfs_count_40percent_landcover3.csv', index_col = 0).fillna(0)
daily40_lc4 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_40percent/{location}_{freq}_dfs_count_40percent_landcover4.csv', index_col = 0).fillna(0)


daily20_lc1_melted = prepare_dfcount_for_plot(daily20_lc1).rename(columns={'dfs_count': 'dfs_count_daily20'})
daily20_lc2_melted = prepare_dfcount_for_plot(daily20_lc2).rename(columns={'dfs_count': 'dfs_count_daily20'})
daily20_lc3_melted = prepare_dfcount_for_plot(daily20_lc3).rename(columns={'dfs_count': 'dfs_count_daily20'})
daily20_lc4_melted = prepare_dfcount_for_plot(daily20_lc4).rename(columns={'dfs_count': 'dfs_count_daily20'})

daily30_lc1_melted = prepare_dfcount_for_plot(daily30_lc1).rename(columns={'dfs_count': 'dfs_count_daily30'})
daily30_lc2_melted = prepare_dfcount_for_plot(daily30_lc2).rename(columns={'dfs_count': 'dfs_count_daily30'})
daily30_lc3_melted = prepare_dfcount_for_plot(daily30_lc3).rename(columns={'dfs_count': 'dfs_count_daily30'})
daily30_lc4_melted = prepare_dfcount_for_plot(daily30_lc4).rename(columns={'dfs_count': 'dfs_count_daily30'})

daily40_lc1_melted = prepare_dfcount_for_plot(daily40_lc1).rename(columns={'dfs_count': 'dfs_count_daily40'})
daily40_lc2_melted = prepare_dfcount_for_plot(daily40_lc2).rename(columns={'dfs_count': 'dfs_count_daily40'})
daily40_lc3_melted = prepare_dfcount_for_plot(daily40_lc3).rename(columns={'dfs_count': 'dfs_count_daily40'})
daily40_lc4_melted = prepare_dfcount_for_plot(daily40_lc4).rename(columns={'dfs_count': 'dfs_count_daily40'})



daily40_lc4_melted


# merge together 

# make 4 dfs depending ont he scenatio 
merge_on = ['year', 'month', 'elevation', 'elevation_bin', 'id']

# ------- landcover1
dfs = [daily20_lc1_melted, daily30_lc1_melted, daily40_lc1_melted]

lc1_daily = dfs[0][merge_on + ['dfs_count_daily20']]
# Iterate over the remaining dataframes (daily30, daily40) and merge them
for i, (df, label) in enumerate(zip(dfs[1:], [30, 40]), start=2):
    unique_col = f'dfs_count_daily{label}'  # Adjust unique column name
    lc1_daily = pd.merge(lc1_daily, df[merge_on + [unique_col]], on=merge_on)

# add dfspot    
lc1_daily['dfspot_count'] = dfspot1_melted.dfs_count

# ------- landcover2
dfs = [daily20_lc2_melted, daily30_lc2_melted, daily40_lc2_melted]

lc2_daily = dfs[0][merge_on + ['dfs_count_daily20']]
# Iterate over the remaining dataframes (daily30, daily40) and merge them
for i, (df, label) in enumerate(zip(dfs[1:], [30, 40]), start=2):
    unique_col = f'dfs_count_daily{label}'  # Adjust unique column name
    lc2_daily = pd.merge(lc2_daily, df[merge_on + [unique_col]], on=merge_on)

lc2_daily['dfspot_count'] = dfspot2_melted.dfs_count

# ------- landcover3
dfs = [daily20_lc3_melted, daily30_lc3_melted, daily40_lc3_melted]

lc3_daily = dfs[0][merge_on + ['dfs_count_daily20']]
# Iterate over the remaining dataframes (daily30, daily40) and merge them
for i, (df, label) in enumerate(zip(dfs[1:], [30, 40]), start=2):
    unique_col = f'dfs_count_daily{label}'  # Adjust unique column name
    lc3_daily = pd.merge(lc3_daily, df[merge_on + [unique_col]], on=merge_on)

lc3_daily['dfspot_count'] = dfspot3_melted.dfs_count

# ------- landcover4
dfs = [daily20_lc4_melted, daily30_lc4_melted, daily40_lc4_melted]

lc4_daily = dfs[0][merge_on + ['dfs_count_daily20']]
# Iterate over the remaining dataframes (daily30, daily40) and merge them
for i, (df, label) in enumerate(zip(dfs[1:], [30, 40]), start=2):
    unique_col = f'dfs_count_daily{label}'  # Adjust unique column name
    lc4_daily = pd.merge(lc4_daily, df[merge_on + [unique_col]], on=merge_on)

lc4_daily['dfspot_count'] = dfspot4_melted.dfs_count



lc4_daily



