import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os 
import functions


# the data is stored on the external ssd 
# the output should go to the local folder 






method = 'once'

landcover_idx = 3
location = 'langtang'

percentile = '20percent'
landcover = f'landcover{landcover_idx}'

column = 'dfs'
freq = 'year'

# path to data
folder_path = f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_{method}/{landcover_idx}landcover_{percentile}/{location}_climate_cut'

output_path = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_{method}/output_{percentile}/'

print('folder with data:\n',folder_path) 
print('  ')
print('output folder:\n',output_path) 
print('   ')
print('landcover id:', landcover_idx)


%%time

elevation = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/downscaling_simple/coordinates_and_elevation_with_labels_{location}.csv')[['cellnr2','band_data']] 

result_df = pd.DataFrame()


# Iterate over the files in the folders
for folder_name in os.listdir(folder_path):
    # skip all csvs and . files
    if folder_name.endswith('.csv'):
        continue
    if folder_name.startswith('.'):
        continue 
    
    file_path = os.path.join(folder_path, folder_name, 'Sediment.out') #create a path to file 
    
    if os.path.isfile(file_path): #check if its there 
        # read file 
        output_df = pd.read_csv(file_path)
        # count dfs per given time 
        output_df = functions.count_dfs_per_time(output_df, column, 'year')
        
        # take the COUNT column 
        name_column = output_df['count']
        # rename the columns 
        column_name = f'{column}_{folder_name}'
        result_df[column_name] = name_column
        result_df.columns = [col[-3:] for col in result_df.columns]


annaul_df_count = functions.add_elevation_to_df_count(result_df, elevation, freq)


annaul_df_count.to_csv(output_path + f'{location}_annual_df_count_{percentile}_{landcover}.csv')











































method = 'once'

landcover_idx = 4
location = 'mustang'

percentile = '40percent'

landcover = f'landcover{landcover_idx}'

column = 'dfs'

freq = 'month'


# path to data on the external ssd 
folder_path = f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_{method}/{landcover_idx}landcover_{percentile}/{location}_climate_cut'

output_folder = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_{method}/output_{percentile}/'



print('folder with data:\n',folder_path) 
print('  ')
print('output folder:\n',output_folder) 
print('   ')
print('landcover id:', landcover_idx)



%%time

elevation = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/downscaling_simple/coordinates_and_elevation_with_labels_{location}.csv')[['cellnr2','band_data']] 


result_df = pd.DataFrame()


# Iterate over the files in the folders
for folder_name in os.listdir(folder_path):
    # skip all csvs and . files
    if folder_name.endswith('.csv'):
        continue
    if folder_name.startswith('.'):
        continue 
    
    file_path = os.path.join(folder_path, folder_name, 'Sediment.out') #create a path to file 
    
    if os.path.isfile(file_path): #check if its there 
        # read file 
        output_df = pd.read_csv(file_path)
        # count dfs per given time 
        output_df = functions.count_dfs_per_time(output_df, column, freq)
        
        # take the COUNT column 
        name_column = output_df['count']
        # rename the columns 
        column_name = f'{column}_{folder_name}'
        result_df[column_name] = name_column
        result_df.columns = [col[-3:] for col in result_df.columns]


df_count = functions.add_elevation_to_df_count(result_df, elevation, freq)




df_count.to_csv(output_folder + f'{location}_monthly_df_count_{percentile}_{landcover}.csv')




# df_count



