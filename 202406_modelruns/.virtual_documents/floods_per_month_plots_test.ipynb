import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import functions





def prepare_dfcount_for_plot(dfcount):
    melted = pd.melt(dfcount, id_vars=['date_id', 'year', 'month'], var_name='elevation', value_name='dfs_count')
    # melted = pd.melt(dfcount, id_vars=['date_id'], var_name='elevation', value_name='dfs_count')
    melted['elevation'] = melted['elevation'].str.split('.').str[0].astype(float)
    melted['elevation_bin'] = melted.apply(functions.bin_elevation500, axis=1)
    # create unique id for elevation and month for merging
    melted['elevation'] = melted['elevation'].astype(int)
    melted['id'] = melted['elevation'].astype(str) + "_" + melted['year'].astype(str)+ "_" + melted['month'].astype(str)
    
    # melted = melted.sort_values('elevation_bin')
    # melted = melted.dropna(subset=['dfs_count'])
    return melted 


# def prepare_dfcount_for_plot(dfcount):
#     melted = pd.melt(dfcount, id_vars=['date_id', 'year', 'month'], var_name='elevation', value_name='dfs_count')
#     melted['elevation'] = melted['elevation'].str.split('.').str[0].astype(float)
#     melted['elevation_bin'] = melted.apply(functions.bin_elevation500, axis=1)
#     # create unique id for elevation and month for merging
#     melted['elevation'] = melted['elevation'].astype(int)
#     # melted['id'] = melted['elevation'].astype(str) + "_" + melted['D_month'].astype(str)
    
#     # melted = melted.sort_values('elevation_bin')
#     # melted = melted.dropna(subset=['dfs_count'])
#     return melted 



freq = 'monthly'
column = 'dfspot'
location = 'langtang'


flood_folder =  f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/dfspot_count/{location}2/'

dfspot1 = pd.read_csv(flood_folder + f'{location}_{freq}_{column}_count_landcover1.csv', index_col = 0).fillna(0)
dfspot2 = pd.read_csv(flood_folder + f'{location}_{freq}_{column}_count_landcover2.csv', index_col = 0).fillna(0)
dfspot3 = pd.read_csv(flood_folder + f'{location}_{freq}_{column}_count_landcover3.csv', index_col = 0).fillna(0)
dfspot4 = pd.read_csv(flood_folder + f'{location}_{freq}_{column}_count_landcover4.csv', index_col = 0).fillna(0)

dfspot1_melted = prepare_dfcount_for_plot(dfspot1)
dfspot2_melted = prepare_dfcount_for_plot(dfspot2)
dfspot3_melted = prepare_dfcount_for_plot(dfspot3)
dfspot4_melted = prepare_dfcount_for_plot(dfspot4)






daily20_lc1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_20percent/{location}_{freq}_dfs_count_20percent_landcover1.csv', index_col = 0).fillna(0)
daily20_lc2 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_20percent/{location}_{freq}_dfs_count_20percent_landcover2.csv', index_col = 0).fillna(0)
daily20_lc3 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_20percent/{location}_{freq}_dfs_count_20percent_landcover3.csv', index_col = 0).fillna(0)
daily20_lc4 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_20percent/{location}_{freq}_dfs_count_20percent_landcover4.csv', index_col = 0).fillna(0)

daily30_lc1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_30percent/{location}_{freq}_dfs_count_30percent_landcover1.csv', index_col = 0).fillna(0)
daily30_lc2 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_30percent/{location}_{freq}_dfs_count_30percent_landcover2.csv', index_col = 0).fillna(0)
daily30_lc3 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_30percent/{location}_{freq}_dfs_count_30percent_landcover3.csv', index_col = 0).fillna(0)
daily30_lc4 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_30percent/{location}_{freq}_dfs_count_30percent_landcover4.csv', index_col = 0).fillna(0)

daily40_lc1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_40percent/{location}_{freq}_dfs_count_40percent_landcover1.csv', index_col = 0).fillna(0)
daily40_lc2 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_40percent/{location}_{freq}_dfs_count_40percent_landcover2.csv', index_col = 0).fillna(0)
daily40_lc3 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_40percent/{location}_{freq}_dfs_count_40percent_landcover3.csv', index_col = 0).fillna(0)
daily40_lc4 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_daily/output_40percent/{location}_{freq}_dfs_count_40percent_landcover4.csv', index_col = 0).fillna(0)


daily20_lc1_melted = prepare_dfcount_for_plot(daily20_lc1).rename(columns={'dfs_count': 'dfs_count_daily20'})
daily20_lc2_melted = prepare_dfcount_for_plot(daily20_lc2).rename(columns={'dfs_count': 'dfs_count_daily20'})
daily20_lc3_melted = prepare_dfcount_for_plot(daily20_lc3).rename(columns={'dfs_count': 'dfs_count_daily20'})
daily20_lc4_melted = prepare_dfcount_for_plot(daily20_lc4).rename(columns={'dfs_count': 'dfs_count_daily20'})

daily30_lc1_melted = prepare_dfcount_for_plot(daily30_lc1).rename(columns={'dfs_count': 'dfs_count_daily30'})
daily30_lc2_melted = prepare_dfcount_for_plot(daily30_lc2).rename(columns={'dfs_count': 'dfs_count_daily30'})
daily30_lc3_melted = prepare_dfcount_for_plot(daily30_lc3).rename(columns={'dfs_count': 'dfs_count_daily30'})
daily30_lc4_melted = prepare_dfcount_for_plot(daily30_lc4).rename(columns={'dfs_count': 'dfs_count_daily30'})

daily40_lc1_melted = prepare_dfcount_for_plot(daily40_lc1).rename(columns={'dfs_count': 'dfs_count_daily40'})
daily40_lc2_melted = prepare_dfcount_for_plot(daily40_lc2).rename(columns={'dfs_count': 'dfs_count_daily40'})
daily40_lc3_melted = prepare_dfcount_for_plot(daily40_lc3).rename(columns={'dfs_count': 'dfs_count_daily40'})
daily40_lc4_melted = prepare_dfcount_for_plot(daily40_lc4).rename(columns={'dfs_count': 'dfs_count_daily40'})







