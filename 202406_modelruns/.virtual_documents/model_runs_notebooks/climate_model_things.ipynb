import pandas as pd
import os
import numpy as np
np.set_printoptions(precision=4)

import catboost
from catboost import *
from catboost import CatBoostClassifier, Pool, metrics, cv
from catboost.utils import get_roc_curve, get_confusion_matrix

import matplotlib.pyplot as plt
import matplotlib.colors as colors
import seaborn as sns

# other things: from sklearn.model_selection import StratifiedShuffleSplit 

from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit, ShuffleSplit
from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score, recall_score, f1_score, confusion_matrix, precision_score
from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK

import shap 

import functions 


# initial auc score 
initial_auc = 0.8986914159732593

best_model_params = {
    'loss_function': 'CrossEntropy', 
    'iterations': 1000,
    'depth': 4,
    'learning_rate': 0.1,
    'eval_metric': 'AUC:hints=skip_train~false',
    # 'eval_metric': metrics.AUC(),
    'custom_metric' : ['Logloss', 'Accuracy', 'AUC:hints=skip_train~false'],
    'random_seed': 42,
    'logging_level': 'Silent',
    'use_best_model': True
}



df = pd.read_csv('/Users/varyabazilova/Desktop/alluvial_fans/final/!!_050723_cathments_data.csv', index_col = 0)
df = df[df['area_m2'] > 1000]


df = df.drop(['x_wgs', 'y_wgs', 'id'], axis = 1)

morphometry = ['elv_median', 'elv_range', 'area_m2', 'perim_m', 'sl_median', 
               'target', 'M', 'circularity_ratio', 'compactness']

climate = ['elv_median', 'elv_range', 'area_m2', 'perim_m', 'sl_median',
           'max_annualsum_tp', 'n_rainydays_median', 'precip95', 'cross_zero',
           'frost_days', 'tp', 'snow', 'rain', 'veg_frac', 'target', 'M',
           'circularity_ratio', 'compactness', 'mean_annual_t2m_downsc',
           'cont_permafrost', 'glacier']


# climate = ['target', 'perim_m', 'area_m2', 'elv_range', 'cross_zero', 'snow', 'mean_annual_t2m_downsc', 'max_annualsum_tp', 'cont_permafrost', 'glacier']

morphometry = df[morphometry]
climate = df[climate]




# df.to_csv('/Users/varyabazilova/Desktop/alluvial_fans/reviews/new_text_final/!!20240830_cathments_data.csv')






y = climate.target
X = climate.drop(columns=['target'])

# # # what values are categorical: 
X['glacier'] = X['glacier'].astype(str) 
X['cont_permafrost'] = X['cont_permafrost'].astype(str)
# X['region'] = X['region'].astype(str)

cat_features = list(['cont_permafrost', 'glacier'])#, 'region'])

seed = 42
test_size = 0.3





model, auc10, accuracy10 = functions.strat_split_data_make_model_save_metrics(X=X, y=y, cat_features=cat_features, 
                                         seed = 42, n_splits=10, model_params=best_model_params)

modelC1, aucC1, accuracyC1 = functions.strat_split_data_make_model_save_metrics(X=X, y=y, cat_features=cat_features, 
                                         seed = 42, n_splits=1, model_params=best_model_params)





np.array(aucC1)



# data split:
climate['target'].value_counts()

# random guess:  
print('random guess chanses with this data split: %.2f%%' % (1068/len(df) * 100)) #df/total 





model, aucReg10, accuracyReg10 = functions.split_data_make_model_save_metrics(X=X, y=y, cat_features=cat_features, 
                                         seed = 42, n_splits=10, model_params=best_model_params)

model, aucReg1, accuracyREG1 = functions.split_data_make_model_save_metrics(X=X, y=y, cat_features=cat_features, 
                                         seed = 42, n_splits=1, model_params=best_model_params)







fig = plt.figure(figsize=(20, 7))
mosaic = fig.subplot_mosaic('''
                            ab
                            ''')
# stratified sampling
mosaic['a'].bar(range(1, len(auc10) + 1), auc10, color='blue', alpha=0.6, label = 'StratifiedSplitShuffle splits')
mosaic['a'].bar(x = 12, height = aucC1, color = plt.cm.PuOr(0.9), alpha = 0.9, label = 'MC-model (ratio was preserved)')
# mosaic['a'].bar(x = 13, height = initial_auc, color = 'pink', label = 'initial (old) model (ratio was not preserved)')
mosaic['a'].bar(x = 13, height = aucReg1, color = 'pink', label = 'MC -model (ratio was not preserved)')

mosaic['a'].axhline(y=np.percentile(auc10, 95), color='green', linestyle = '--', label = '5/95 %')
mosaic['a'].axhline(y=np.percentile(auc10, 5), color='green', linestyle = '--')
mosaic['a'].axhline(y=np.mean(auc10), color = 'red', linestyle = '--', label = 'mean')



# regular sampling
mosaic['b'].bar(range(1, len(aucReg10) + 1), aucReg10, color='blue', alpha=0.7, label = 'SplitShuffle split')
# mosaic['b'].bar(x = 12, height = aucC1, color = 'orange', label = 'new model with ')
mosaic['b'].bar(x = 13, height = aucReg1, color = 'pink', label = 'MC-model (ratio was not preserved)')

mosaic['b'].axhline(y=np.percentile(aucReg10, 95), color='green', linestyle = '--', label = '5/95 %')
mosaic['b'].axhline(y=np.percentile(aucReg10, 5), color='green', linestyle = '--')
mosaic['b'].axhline(y=np.mean(aucReg10), color = 'red', linestyle = '--', label = 'mean')


# mosaic['a'].set_ylim(0.875, 1) 
# mosaic['b'].set_ylim(0.875, 1) 
# mosaic['a'].legend(fontsize=12)
# mosaic['b'].legend(fontsize=12)


# mosaic['a'].set_title('StratifiedShuffleSplit: the proportion of data points \n of class 1/0 is preserved, when splitting the data', fontsize = 15)
# mosaic['b'].set_title('ShuffleSplit: the data is split randomly', fontsize = 15)

# mosaic['a'].set_xticks(range(1, len(aucReg10) + 1), fontsize = 15)
# mosaic['b'].set_xticks(range(1, len(aucReg10) + 1), fontsize = 15)

# mosaic['a'].set_ylabel('auc', fontsize = 20)
# mosaic['b'].set_ylabel('auc', fontsize = 20)


# mosaic['a'].set_xlabel('split number (1-10)', fontsize = 15)
# mosaic['b'].set_xlabel('split number (1-10)', fontsize = 15)






mosaic['a'].set_ylim(0.84, 1) 
mosaic['b'].set_ylim(0.84, 1) 
mosaic['a'].legend(fontsize=12)
mosaic['b'].legend(fontsize=12)


mosaic['a'].set_title('StratifiedShuffleSplit: the proportion of data points \n of class 1/0 is preserved, when splitting the data', fontsize = 15)
mosaic['b'].set_title('ShuffleSplit: the data is split randomly', fontsize = 15)

mosaic['a'].set_xticks(range(1, len(aucReg10) + 1))#, fontsize = 20)
mosaic['b'].set_xticks(range(1, len(aucReg10) + 1))#, fontsize = 20)
mosaic['a'].set_xticklabels(range(1, len(aucReg10) + 1), fontsize=15)
mosaic['b'].set_xticklabels(range(1, len(aucReg10) + 1), fontsize=15)


mosaic['a'].set_ylabel('auc', fontsize = 15)
mosaic['b'].set_ylabel('auc', fontsize = 15)


mosaic['a'].set_xlabel('split number (1-10)', fontsize = 15)
mosaic['b'].set_xlabel('split number (1-10)', fontsize = 15)



mosaic['a'].set_ylabel('auc', fontsize = 20)
mosaic['b'].set_ylabel('auc', fontsize = 20)


mosaic['a'].set_xlabel('split number (1-10)', fontsize = 15)
mosaic['b'].set_xlabel('split number (1-10)', fontsize = 15)

mosaic['a'].set_yticklabels(labels=np.round(np.arange(0.84, 1.02, 0.02), 2), fontsize=15)
mosaic['b'].set_yticklabels(labels=np.round(np.arange(0.84, 1.02, 0.02), 2), fontsize=15)







# plt.show()

# plt.savefig('out/removed_points/StratifiedShuffleSplit_together_with_regular_paper_MC_model.png', dpi = 300, bbox_inches = 'tight')


aucC1





fig = plt.figure(figsize=(10, 7))
mosaic = fig.subplot_mosaic('''
                            a
                            ''')
# stratified sampling
mosaic['a'].bar(range(1, len(auc10) + 1), auc10, color='blue', alpha=0.6, label = 'StratifiedSplitShuffle splits')
mosaic['a'].bar(x = 12, height = aucC1, color = plt.cm.PuOr(0.9), alpha = 0.9, label = 'MC-model (ratio was preserved)')
# mosaic['a'].bar(x = 13, height = initial_auc, color = 'pink', label = 'initial (old) model (ratio was not preserved)')
mosaic['a'].bar(x = 13, height = aucReg1, color = 'pink', label = 'MC -model (ratio was not preserved)')

mosaic['a'].axhline(y=np.percentile(auc10, 95), color='green', linestyle = '--', label = '5/95 %')
mosaic['a'].axhline(y=np.percentile(auc10, 5), color='green', linestyle = '--')
mosaic['a'].axhline(y=np.mean(auc10), color = 'red', linestyle = '--', label = 'mean')

mosaic['a'].bar(x = 14, height = 0.881, color = 'orange', label = 'M- model (ratio was preserved)')


# # regular sampling
# mosaic['b'].bar(range(1, len(aucReg10) + 1), aucReg10, color='blue', alpha=0.7, label = 'SplitShuffle split')
# # mosaic['b'].bar(x = 12, height = aucC1, color = 'orange', label = 'new model with ')
# mosaic['b'].bar(x = 13, height = aucReg1, color = 'pink', label = 'MC-model (ratio was not preserved)')

# mosaic['b'].axhline(y=np.percentile(aucReg10, 95), color='green', linestyle = '--', label = '5/95 %')
# mosaic['b'].axhline(y=np.percentile(aucReg10, 5), color='green', linestyle = '--')
# mosaic['b'].axhline(y=np.mean(aucReg10), color = 'red', linestyle = '--', label = 'mean')



mosaic['a'].set_ylim(0.84, 1) 
# mosaic['b'].set_ylim(0.84, 1) 
mosaic['a'].legend(fontsize=12)
# mosaic['b'].legend(fontsize=12)


# mosaic['a'].set_title('StratifiedShuffleSplit: the proportion of data points \n of class 1/0 is preserved, when splitting the data', fontsize = 15)
# mosaic['b'].set_title('ShuffleSplit: the data is split randomly', fontsize = 15)

mosaic['a'].set_xticks(range(1, len(aucReg10) + 1))#, fontsize = 20)
# mosaic['b'].set_xticks(range(1, len(aucReg10) + 1))#, fontsize = 20)
mosaic['a'].set_xticklabels(range(1, len(aucReg10) + 1), fontsize=15)
# mosaic['b'].set_xticklabels(range(1, len(aucReg10) + 1), fontsize=15)


mosaic['a'].set_ylabel('auc', fontsize = 15)
# mosaic['b'].set_ylabel('auc', fontsize = 15)


mosaic['a'].set_xlabel('split number (1-10)', fontsize = 15)
# mosaic['b'].set_xlabel('split number (1-10)', fontsize = 15)



mosaic['a'].set_ylabel('auc', fontsize = 20)
# mosaic['b'].set_ylabel('auc', fontsize = 20)


mosaic['a'].set_xlabel('split number (1-10)', fontsize = 15)
# mosaic['b'].set_xlabel('split number (1-10)', fontsize = 15)

mosaic['a'].set_yticklabels(labels=np.round(np.arange(0.84, 1.02, 0.02), 2), fontsize=15)
# mosaic['b'].set_yticklabels(labels=np.round(np.arange(0.84, 1.02, 0.02), 2), fontsize=15)



# plt.savefig('out/removed_points/StratifiedShuffleSplit_together_with_morph_main_text_paper.png', dpi = 300, bbox_inches = 'tight')





# make predictions 
model = modelC1[0]

y_result = model.predict(X)
probs = model.predict_proba(X)
probs = probs[:,1]


# calculate confusions 

df['y_result'] = y_result
df['y_result_probs'] = probs

df['result_True'] = df.target + df.y_result # TP = 2, TN = 0


# this is the column to see the "confusuon" situations
# TP = 2, TN = 0
# FP = -1, FN = 1
df['diff_res'] = np.where(df.result_True != 1, df.result_True, (df.target - df.y_result)) 

# Then, create a new column 'confusion' based on the condition 'diff_res' == 1
df['confusion'] = np.where(df['diff_res'] == 1, 'FN',  # False Negative
                           np.where(df['diff_res'] == 2, 'TP',  # True Positive
                                    np.where(df['diff_res'] == -1, 'FP',  # False Positive
                                             np.where(df['diff_res'] == 0, 'TN', 'Unknown'))))  # True Negative and Unknown

# save predictions
# df.to_csv('out/removed_points/20240711_new_model_predictions_output.csv')

# test what will happen if i remove all the glaciers 
# df.to_csv('out/removed_points/20240711_new_model_predictions_output_test_no_glaciers.csv')


df.target.value_counts()



correct = 992 + 645
wrong = 93 + 76

total = len(df)

percent_correct = correct /total 
percent_wrong = wrong / total

print('fraction of wrong classification:', np.round((percent_wrong * 100),2))


accuracyC1








df_predict = pd.read_csv('out/removed_points/20240711_new_model_predictions_output.csv', index_col = 0)



explainer = shap.TreeExplainer(model)

shap_values = explainer.shap_values(X)
shap.summary_plot(shap_values, X, max_display=21, show=False)



# shap.summary_plot(shap_values, X, show=False, cmap=cut_cmap, max_display = 8,plot_size=(10, 7))


features = ['median slope', 'perimeter','area','relief',  
            'mean annual temperature', 'snowfall', 'thermal weathering', 'Melton ratio', 
            'max annual precipitation',
            'circularity ratio', 
            'rainfall',
            'N of wet days',
            'frost weathering',
            'median elevation',
            '95% precipitation',
            'compactness',  
            'total precipitation', 'vegetation cover (%)', 'glacier', 'continious permafrost'] # 1 purple

my_colors = ['orange', 'orange','orange','orange',  
            'purple', 'purple', 'purple', 
            'orange', 
            'purple',
            'orange', 
            'purple',
            'purple',
            'purple',
            'orange',
            'purple',
            'orange',  
            'purple', 'purple', 'purple', 'purple'] # 1 purple

features = ['median slope', 'perimeter','area','relief',  
            'mean annual temperature', 'snowfall', 'thermal weathering', 'Melton ratio']


purple = plt.cm.PuOr(0.9)
orange = plt.cm.PuOr(0.125)



my_colors = [orange, orange,orange,orange,  
            purple, purple, purple, orange]

def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):
    new_cmap = colors.LinearSegmentedColormap.from_list(
        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),
        cmap(np.linspace(minval, maxval, n)))
    return new_cmap


cmap = plt.get_cmap('bone_r')
cut_cmap = truncate_colormap(cmap, 0.1, 1)



features = ['median slope', 'perimeter','area','relief',  
            'mean annual temperature', 'snowfall', 'thermal weathering', 'Melton ratio']

my_colors = [orange, orange,orange,orange,  
            purple, purple, purple, orange]
# shap.summary_plot(shap_values, X, show=False, cmap=cut_cmap, plot_size=(10, 12))

shap.summary_plot(shap_values, X, show=False, cmap=cut_cmap, max_display = 8,plot_size=(10, 7))


# color and rename labels 
for ticklabel, tickcolor in zip(plt.gca().get_yticklabels(), my_colors[::-1]):
    ticklabel.set_color(tickcolor)

plt.yticks(range(len(features)), features[::-1], fontsize=15)
# plt.yticks(range(len(features)), features, fontsize=15) # this is upside down
plt.xticks(fontsize=15)

# plt.savefig('out/removed_points/Cmodel_first8_shap_summaryplot.png', dpi = 300, bbox_inches = 'tight')


# SAVE SHAP 
shapdf = pd.DataFrame(shap_values, columns=X.columns)
shapdf = shapdf.abs()
shapdf['region'] = df['region']

# Apply the function to create the new column
shapdf['big_region'] = shapdf.apply(functions.categorize_region, axis=1)
shapdf_mean = shapdf.groupby('big_region').mean()








shapdf['big_region'].value_counts()












# make colors: 

x = np.arange(0, 9, 1)
browns = x/(x.max()*2)

x2 = np.arange(1, 13, 1)
purples = x2/(x2.max()*2) + 0.5

x_combined = np.hstack([browns[:-1], purples])

colors = []
for i in x_combined:
    col = plt.cm.PuOr(i)
    colors.append(col)

# colors for each column
column_colors = {
    '(1) median elevation': colors[0], 
    '(2) relief': colors[1], 
    '(3) area': colors[2], 
    '(4) perimeter': colors[3], 
    '(5) median slope': colors[4],
    '(6) Melton ratio': colors[5], 
    '(7) circularity ratio': colors[6], 
    '(8) compactness': colors[7],
    '(9) total annual precipitation': colors[8], 
    '(10) max annual precipitation': colors[9],
    '(11) N of wet days': colors[10], 
    '(12) 95% precipitation': colors[11], 
    '(13) snowfall': colors[12], 
    '(14) rainfall': colors[13],
    '(15) mean annual temperature': colors[14], 
    '(16) thermal weathering': colors[15], 
    '(17) frost weathering': colors[16],
    '(18) vegetation cover (%)': colors[17],
    '(19) continious permafrost': colors[18], 
    '(20) glacier': colors[19]
}


# organize data for plotting  
df = shapdf_mean

df = functions.reorder_columns_for_colors(df)
df = functions.rename_columns_for_colors(df)




flattened = df.stack().reset_index()
flattened.columns = ['Row', 'Column', 'Value']
sorted_flattened = flattened.groupby('Row').apply(lambda x: x.sort_values(by='Value', ascending=False)).reset_index(drop=True)

# Add gaps to the index
gap_size = 1 # Number of gaps between groups
new_index = []
new_labels = []

for i, (row, group) in enumerate(sorted_flattened.groupby('Row')):
    start_index = i * (len(group) + gap_size)
    end_index = start_index + len(group)
    new_index.extend(range(start_index, end_index))
    new_labels.extend([col for col in group['Column']])
    # new_labels.extend([f"{row}-{col}" for col in group['Column']])


sorted_flattened.index = new_index

# Define the size of the figure
plt.figure(figsize=(30, 17))

# Set the width of the bars
bar_width = 0.9  # Adjust this value as needed

# Plot the data with adjusted bar positions
bars = plt.bar(sorted_flattened.index, sorted_flattened['Value'], width=bar_width,
               color=[column_colors[col] for col in sorted_flattened['Column']])

# Create custom legend
legend_handles = [plt.Line2D([0], [0], color=color, lw=4) for color in column_colors.values()]
# plt.legend(legend_handles, column_colors.keys(), fontsize=15, bbox_to_anchor=(0.92, 0.85))

# plt.legend(legend_handles, column_colors.keys(), fontsize=15, loc='upper right', bbox_to_anchor=(1.05, 0.9))  # Adjusted value
# loc="lower right",ncol=4)

plt.legend(legend_handles, column_colors.keys(), fontsize=23, loc='upper center', ncol = 5, bbox_to_anchor=(0.5, 1.21))  


plt.ylabel('mean(|shap|) \n relative contribution of feature X(i) to the model output', fontsize=22)



rotation = 45

if rotation == 0:
    ha = 'center'
    va = 'top'
elif rotation == 90:
    ha = 'right'
    va = 'top'
else:
    ha = 'right'
    va = 'top'

plt.xticks(ticks=new_index, labels=new_labels, rotation=rotation, ha='right', rotation_mode='anchor', fontsize = 14)
plt.yticks(fontsize=22)

# Adjust margins to reduce space on the sides
plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.1)

# Adjust x-axis limits to leave more space on the sides
plt.xlim(sorted_flattened.index.min() - 1, sorted_flattened.index.max() + 1)

plt.tight_layout()
# plt.show()

group_labels = ['East', 'Interior', 'North', 'South', 'West']
# group_labels = ['one', 'two', 'three', 'four']
# Determine the y position for the labels to align them horizontally
max_height = sorted_flattened['Value'].max()
label_y_position = max_height - 0.02  # Adjust this value if needed

for i, (row, group) in enumerate(sorted_flattened.groupby('Row')):
    start_index = i * (len(group) + gap_size)
    end_index = start_index + len(group) - 1

    plt.text((start_index + end_index) / 2, label_y_position, group_labels[i % len(group_labels)], 
             ha='center', va='bottom', fontsize=30)

# plt.savefig('out/removed_points/sorted_shap_barchart_per_big_region.png', dpi = 300, bbox_inches = 'tight')





morphometry_part = ['elv_median', 'elv_range', 'area_m2', 'perim_m', 'sl_median', 
                    'M', 'circularity_ratio', 'compactness']

climate_part = ['max_annualsum_tp', 'n_rainydays_median', 'precip95', 'cross_zero',
                'frost_days', 'tp', 'snow', 'rain', 'veg_frac','mean_annual_t2m_downsc',
                'cont_permafrost', 'glacier']


morphometry_part = shapdf_mean[morphometry_part]
climate_part = shapdf_mean[climate_part]


morphometry_part = morphometry_part.T
morphometry_sum = morphometry_part.sum()

climate_part = climate_part.T
climate_sum = climate_part.sum()


totalshap = pd.DataFrame()
totalshap['morph'] = morphometry_sum
totalshap['clim'] = climate_sum


# totalshap['total'] = totalshap.morph + totalshap.clim

totalshap2 = totalshap.T
totalshap2


totalshap2.West.plot.pie(figsize=(7,7), colors = [orange, purple],startangle = 90 )
# plt.savefig('out/removed_points/shap_pie_west.png', dpi = 300, bbox_inches = 'tight')


totalshap['total'] = totalshap.morph + totalshap.clim

totalshap['clim_frac'] = totalshap.clim * 100 / totalshap.total


totalshap





# decode correct and wrong to 1 and 0
df_predict = pd.read_csv('out/20240605_new_model_predictions_output.csv', index_col = 0)

# bin the probability for the plot
bin_labels = []
for i in range(0, 100, 5):
    label = f"{i}-{i+5}"
    bin_labels.append(label)

col_wr = plt.cm.bone(1/8)
col_correct = plt.cm.bone(7/8)


df_predict['quality'] = df_predict.confusion.apply(functions.determine_class)

df_predict['bin'] = df_predict.y_result_probs.apply(functions.determine_probability_bin)
df_predict['order'] = df_predict.y_result_probs.apply(functions.determine_order_for_bins_plot)



# filter
correct = df_predict[df_predict.quality == 1].groupby(['order']).count()['quality']
wrong = df_predict[df_predict.quality == 0].groupby(['order']).count()['quality']



alltogether = pd.DataFrame()
alltogether['correctly classified'] = correct
alltogether['wrongly classified'] = wrong
alltogether['total'] = alltogether['correctly classified'] + alltogether['wrongly classified']
alltogether['percent_correct'] = (alltogether['correctly classified'] * 100) / alltogether.total
alltogether['percent_wrong'] = (alltogether['wrongly classified'] * 100) / alltogether.total

alltogether.sort_values('percent_wrong', ascending = False)



forplot = pd.DataFrame()
forplot['correctly classified'] = alltogether.percent_correct
forplot['wrongly classified'] = alltogether.percent_wrong





plt.rcParams.update({'font.size': 20})

colors = [col_correct, col_wr]

# filter
# forplot = alltogether[['correctly classified', 'wrongly classified']]
# forplot = alltogether[['percent_correct', 'percent_wrong']]

# plot
fig, ax = plt.subplots()
forplot.plot.bar(figsize = (25, 8),  alpha = 0.8, rot=45, width=0.8, 
                                                          stacked=True, edgecolor = 'black', ax=ax, color = colors)


plt.xticks(range(len(bin_labels)), bin_labels, rotation=45)  # Set the x-axis labels
plt.ylabel('[%]')
plt.xlabel('probability [%]')
# plt.xlabel('[%]')
plt.show()


# fig.savefig('out/barplot_wrong_correct_with_percent.png', dpi = 300, bbox_inches = 'tight')#, transparent = True)









# feature correlation:

X = functions.reorder_columns_for_colors(X)
X = functions.rename_columns_for_colors(X)
X['target'] = climate.target

x_columns = X.columns 




# X


# X['(19) continious permafrost']


# X['(20) glacier'].astype(int)
X['(20) glacier'] = X['(20) glacier'].astype(int)
X['(19) continious permafrost'] = X['(19) continious permafrost'].astype(int)


def correlation_heatmap(X):
    correlations = X.corr()

    fig, ax = plt.subplots(figsize=(20,20))
    heatmap = sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f', cmap="seismic", alpha = 0.7,
                          square=True, linewidths=.5, annot=False, cbar_kws={"shrink": .70})
    
    # Get the current tick labels
    labels = heatmap.get_yticklabels()
    
    # Color the first 5 labels orange, the rest purple
    for label in labels[:8]:
        label.set_color('darkorange')
    for label in labels[8:]:
        label.set_color('purple')
    for label in labels[-1:]:
        label.set_color('black')
    
    heatmap.set_yticklabels(labels)
    heatmap.set_yticklabels(labels)
    
    # Get the current tick labels
    labels = heatmap.get_xticklabels()
    
    # Color the first 5 labels orange, the rest purple
    for label in labels[:8]:
        label.set_color('darkorange')
    for label in labels[8:]:
        label.set_color('purple')
    for label in labels[-1:]:
        label.set_color('black')
    
    heatmap.set_xticklabels(labels)
    heatmap.set_xticklabels(labels)

    # plt.show()
correlation_heatmap(X[x_columns])


# plt.savefig('out/removed_points/correlation_matrix_all_features.png', dpi = 300, bbox_inches = 'tight')


X.columns








#define functions to make scales for a plot: 

#### LOG SCALE ####
def make_log_x_scale(data, column, ax):
    ax.hist(data[data.target==0][column].apply(np.log10), bins=20, color=ff_color, alpha = 0.5
           , weights=100*np.ones(len(data[data.target==0])) / len(data[data.target==0]))
    ax.hist(data[data.target==1][column].apply(np.log10), bins=20, color=df_color, alpha=0.5
            , weights=100*np.ones(len(data[data.target==1])) / len(data[data.target==1]))
    ax.set_xlabel(f'log({column})')
    ax.set_ylabel('[%]')


#### LINEAR SCALE ####
def make_linear_x_scale(data, column, ax):
    ax.hist(data[data.target==0][column], bins=20, color=ff_color, alpha = 0.5#, edgecolor='black', alpha = 0.5#, label='FF'
            , weights=100*np.ones(len(data[data.target==0])) / len(data[data.target==0]))
    
    ax.hist(data[data.target==1][column], bins=20, color=df_color,  alpha = 0.5#, edgecolor='black', alpha=0.25#, label='DF'
           , weights=100*np.ones(len(data[data.target==1])) / len(data[data.target==1]))
    # ax.legend()
    ax.set_ylabel('[%]')
    ax.set_xlabel(f'{column}')

    
ff_color = plt.cm.Set1(1/8)
df_color = plt.cm.Set1(0/8)


together = pd.read_csv('/Users/varyabazilova/Desktop/alluvial_fans/final/!!_050723_cathments_data.csv', index_col = 0)
together = together[together['area_m2'] > 1000]


together = together.drop(['x_wgs', 'y_wgs', 'id'], axis = 1)



# Plot with Mosaic

plt.rcParams.update({'font.size': 20})



fig = plt.figure(figsize=(25, 25))
mosaic = fig.subplot_mosaic('''
                            2345
                            6789
                            jklm
                            nopq
                            rstu
                            ''')

# ------ morphometric: 

# normal scale-=
make_linear_x_scale(data=together, column='elv_median',        ax=mosaic['2'])
mosaic['2'].set(xlabel="median elevation")
make_linear_x_scale(data=together, column='elv_range',         ax=mosaic['3'])
mosaic['3'].set(xlabel="relief")
make_linear_x_scale(data=together, column='sl_median',         ax=mosaic['4'])
mosaic['4'].set(xlabel="median slope")
make_linear_x_scale(data=together, column='circularity_ratio', ax=mosaic['5'])
mosaic['5'].set(xlabel="circularity ratio")
make_linear_x_scale(data=together, column='compactness',       ax=mosaic['6'])
mosaic['6'].set(xlabel="compactness")

# log scale
make_log_x_scale(data=together, column='area_m2', ax=mosaic['7'])
mosaic['7'].set(xlabel="log(area [m2])")
make_log_x_scale(data=together, column='perim_m', ax=mosaic['8'])
mosaic['8'].set(xlabel="log(perimeter [m])")
make_log_x_scale(data=together, column='M',       ax=mosaic['9'])
mosaic['9'].set(xlabel="log(Melton ratio (M))")


# mark with *
# mosaic['2'].scatter([10], [13.5],   marker = 'o', s = 1000, facecolors= plt.cm.PuOr(2.1/8), edgecolors='none') 
# mosaic['3'].scatter([10], [13.5],   marker = 'o', s = 1000, facecolors= plt.cm.PuOr(2.1/8), edgecolors='none') 
# mosaic['4'].scatter([0], [13.5],    marker = 'o', s = 1000, facecolors= plt.cm.PuOr(2.1/8), edgecolors='none') 
# mosaic['5'].scatter([0.1], [17.1],  marker = 'o', s = 1000, facecolors= plt.cm.PuOr(2.1/8), edgecolors='none') 
# mosaic['6'].scatter([1.1], [19],    marker = 'o', s = 1000, facecolors= plt.cm.PuOr(2.1/8), edgecolors='none') 
# mosaic['7'].scatter([2.5], [16.2],  marker = 'o', s = 1000, facecolors= plt.cm.PuOr(2.1/8), edgecolors='none') 
# mosaic['8'].scatter([2], [18.2],    marker = 'o', s = 1000, facecolors= plt.cm.PuOr(2.1/8), edgecolors='none') 
# mosaic['9'].scatter([1], [18],      marker = 'o', s = 1000, facecolors= plt.cm.PuOr(2.1/8), edgecolors='none') 





# ------ climate: 
'''
'max_annualsum_tp', 'n_rainydays_median', 'precip95', 'cross_zero',
       'frost_days', 'tp', 'snow', 'rain', 'veg_frac',
       'mean_annual_t2m_downsc', 'cont_permafrost', 'glacier'],'''

# normal scale
# temp
make_linear_x_scale(data=together, column='mean_annual_t2m_downsc', ax=mosaic['j'])
mosaic['j'].set(xlabel="mean annual temperature (C)")
make_linear_x_scale(data=together, column='cross_zero',             ax=mosaic['k'])
mosaic['k'].set(xlabel="thermal weathering *")
make_linear_x_scale(data=together, column='frost_days',             ax=mosaic['l'])
mosaic['l'].set(xlabel="frost weathering **")




# precip
make_linear_x_scale(data=together, column='tp',                     ax=mosaic['m'])
mosaic['m'].set(xlabel="annual precipitation [mm]")
make_linear_x_scale(data=together, column='max_annualsum_tp',       ax=mosaic['n'])
mosaic['n'].set(xlabel="max annual precipitation [mm]")
make_linear_x_scale(data=together, column='precip95',               ax=mosaic['o'])
mosaic['o'].set(xlabel="95 % precipitation [mm]")
make_linear_x_scale(data=together, column='snow',                   ax=mosaic['p'])
mosaic['p'].set(xlabel="snowfall [mm]")
make_linear_x_scale(data=together, column='rain',                   ax=mosaic['q'])
mosaic['q'].set(xlabel="rainfall [mm]")


# log scale
make_log_x_scale(data=together, column='n_rainydays_median',            ax=mosaic['r'])
mosaic['r'].set(xlabel="log(N of wet days)")
make_log_x_scale(data=together[together.veg_frac>0], column='veg_frac', ax=mosaic['s'])
mosaic['s'].set(xlabel="log(vegetated area [%])")



### glaciers ####
g0 = together[together.target==0].groupby(['glacier']).count()['target']
g1 = together[together.target==1].groupby(['glacier']).count()['target']
pd.DataFrame({'FF':g0, 'DF':g1}).plot.bar(ax=mosaic['t'], color=[ff_color, df_color], alpha = 0.5, legend=False)
mosaic['t'].set_xticklabels(['no', 'yes'], rotation = 45)
mosaic['t'].set(ylabel="count")


### permafrost ####
p0 = together[together.target==0].groupby(['cont_permafrost']).count()['target']
p1 = together[together.target==1].groupby(['cont_permafrost']).count()['target']
pd.DataFrame({'FF':p0, 'DF':p1}).plot.bar(ax=mosaic['u'], color=[ff_color, df_color], alpha = 0.5)
mosaic['u'].set(xlabel="continious permafrost")
mosaic['u'].set_xticklabels(['no', 'yes'], rotation = 45)
mosaic['u'].set(ylabel='count')
mosaic['u'].legend(["Flood", "Debris flow"])


# Define labels for each subplot
labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)', '(h)', '(i)',
          '(j)', '(k)', '(l)', '(m)', '(n)', '(o)', '(p)', '(q)', '(r)', 
          '(s)', '(t)']


# Add labels to each subplot
for i, (key, ax) in enumerate(mosaic.items()):
    if i == len(mosaic) - 1 or i == len(mosaic) - 3:  # Check if it's the last or third from the end subplot
        ax.text(0.02, 0.95, labels[i], transform=ax.transAxes,
                fontsize=25, verticalalignment='top', horizontalalignment='left')
    else:
        ax.text(0.98, 0.95, labels[i], transform=ax.transAxes,
                fontsize=25, verticalalignment='top', horizontalalignment='right')


fig.tight_layout()



# SAVE THIS FIGURE, CHANGE THE FONT SIZE 

# plt.savefig('out/removed_points/together_histograms.png', dpi = 300, bbox_inches = 'tight')





















df_predict = pd.read_csv('out/removed_points/20240711_new_model_predictions_output.csv', index_col = 0)
df_predict.columns


colors = {0: 'blue', 1: 'red'}

fig = plt.figure(figsize=(15, 10))

mosaic = fig.subplot_mosaic('''
                            ac
                            bd
                            bd
                            ''')

sns.boxplot(data=df_predict, x='sl_median', y='target',orient='h', ax =mosaic['a'], palette = ['blue', 'red'])
mosaic['a'].set_xticks([])
mosaic['a'].set_xlabel(' ')
mosaic['a'].set_ylabel('class')

# Define the colors

# Scatter plot
for target, color in colors.items():
    subset = df_predict[df_predict['target'] == target]
    mosaic['b'].scatter(subset['sl_median'], subset['y_result_probs'], c=color, label=f'Target {target}', alpha=0.6)
    
mosaic['b'].axhline(0.5, color = 'darkgrey')
# Adding labels and title
mosaic['b'].set_xlabel('slope Median')
mosaic['b'].set_ylabel('calculated Probabilities')
# plt.title('Scatter Plot of SL Median vs Result Probabilities')


sns.boxplot(data=df_predict, x='area_m2', y='target',orient='h', ax =mosaic['c'], palette = ['blue', 'red'])
mosaic['c'].set_xticks([])
mosaic['c'].set_xlabel(' ')
mosaic['c'].set_ylabel('class')

mosaic['c'].set_xscale('log')

# Scatter plot
for target, color in colors.items():
    subset = df_predict[df_predict['target'] == target]
    mosaic['d'].scatter(subset['area_m2'], subset['y_result_probs'], c=color, label=f'class {target}', alpha=0.6)
    
mosaic['d'].axhline(0.5, color = 'darkgrey')
mosaic['d'].set_xscale('log')
mosaic['d'].set_xlabel('log(area m2)')
mosaic['d'].set_ylabel('calculated Probabilities')

plt.legend()


# plt.savefig('out/removed_points/test_histograms.png', dpi = 300, bbox_inches = 'tight')








# 1806*0.7


1806-1264


df_predict


df.columns





df = pd.read_csv('/Users/varyabazilova/Desktop/alluvial_fans/final/!!_050723_cathments_data.csv', index_col = 0)
df = df[df['area_m2'] > 1000]


df = df.drop(['x_wgs', 'y_wgs', 'id'], axis = 1)

morphometry = ['elv_median', 'elv_range', 'area_m2', 'perim_m', 'sl_median', 
               'target', 'M', 'circularity_ratio', 'compactness']

climate = ['max_annualsum_tp', 'n_rainydays_median', 'precip95', 'cross_zero',
           'frost_days', 'tp', 'snow', 'rain', 'target', 'mean_annual_t2m_downsc']

morphometry = df[morphometry]
climate = df[climate]



morphometry_df = morphometry[morphometry['target'] == 1]
morphometry_ff = morphometry[morphometry['target'] == 0]

morphometry_df = functions.rename_columns_morph(morphometry_df)
morphometry_ff = functions.rename_columns_morph(morphometry_ff)

climate_df = climate[climate['target'] == 1]
climate_ff = climate[climate['target'] == 0]

climate_df = functions.rename_columns_clim(climate_df)
climate_ff = functions.rename_columns_clim(climate_ff)

morphometry_df = morphometry_df.drop(['target'], axis = 1)
morphometry_ff = morphometry_ff.drop(['target'], axis = 1)

climate_df = climate_df.drop(['target'], axis = 1)
climate_ff = climate_ff.drop(['target'], axis = 1)


morphometry_df = round(morphometry_df.describe(), 2)
morphometry_df = morphometry_df.drop(['count'], axis = 0)
# morphometry_df.to_csv('out/removed_points/morphometry_df.csv')

morphometry_ff = morphometry_ff.describe()
morphometry_ff = round(morphometry_ff.describe(), 2)
morphometry_ff = morphometry_ff.drop(['count'], axis = 0)
# morphometry_ff.to_csv('out/removed_points/morphometry_ff.csv')



climate_df = round(climate_df.describe(), 2)
climate_df = climate_df.drop(['count'], axis = 0)
# climate_df.to_csv('out/removed_points/climate_df.csv')

climate_ff = climate_ff.describe()
climate_ff = round(climate_ff.describe(), 2)
climate_ff =climate_ff.drop(['count'], axis = 0)
# climate_ff.to_csv('out/removed_points/climate_ff.csv')










df['big_region'] = df.apply(functions.categorize_region, axis=1)
df.columns


df_reg = df.groupby('big_region').mean()
df_reg2 = df_reg.copy()


precip = df_reg2[['tp', 'rain', 'snow']]
precip['rain_frac'] = precip.rain * 100 / precip.tp
precip['snow_frac'] = precip.snow * 100 / precip.tp





precip


dfnorth = df[df['big_region'] == 'North']
dfnorth.region.unique()


dfwest = df[df['big_region'] == 'West']



dfwest.target.value_counts()














# df_predict


df_predict_glaciers = df_predict[df_predict['glacier'] == 1]



df_predict_glaciers_change = df_predict_glaciers

df_predict_glaciers_change['glacier'] = df_predict_glaciers_change['glacier'].replace(1, 0)





climate = ['elv_median', 'elv_range', 'area_m2', 'perim_m', 'sl_median',
           'max_annualsum_tp', 'n_rainydays_median', 'precip95', 'cross_zero',
           'frost_days', 'tp', 'snow', 'rain', 'veg_frac', 'target', 'M',
           'circularity_ratio', 'compactness', 'mean_annual_t2m_downsc',
           'cont_permafrost', 'glacier']


glaciers_changed = df_predict_glaciers_change[climate]

y = glaciers_changed.target
X = glaciers_changed.drop(columns=['target'])

# # # what values are categorical: 
X['glacier'] = X['glacier'].astype(str) 
X['cont_permafrost'] = X['cont_permafrost'].astype(str)
# X['region'] = X['region'].astype(str)

# cat_features = list(['cont_permafrost', 'glacier'])#, 'region'])



# make predictions 
model = modelC1[0]

y_result_gl = model.predict(X)
probs_gl = model.predict_proba(X)
probs_gl = probs_gl[:,1]


# # calculate confusions 
df = glaciers_changed
df['y_result_gl'] = y_result_gl
df['y_result_probs_gl'] = probs_gl

df['result_True'] = df.target + df.y_result_gl # TP = 2, TN = 0


# this is the column to see the "confusuon" situations
# TP = 2, TN = 0
# FP = -1, FN = 1
df['diff_res'] = np.where(df.result_True != 1, df.result_True, (df.target - df.y_result_gl)) 

# Then, create a new column 'confusion' based on the condition 'diff_res' == 1
df['confusion_gl'] = np.where(df['diff_res'] == 1, 'FN',  # False Negative
                           np.where(df['diff_res'] == 2, 'TP',  # True Positive
                                    np.where(df['diff_res'] == -1, 'FP',  # False Positive
                                             np.where(df['diff_res'] == 0, 'TN', 'Unknown'))))  # True Negative and Unknown

# # save predictions
# # df.to_csv('out/removed_points/20240711_new_model_predictions_output.csv')

# # test what will happen if i remove all the glaciers 
# df.to_csv('out/removed_points/20240723_new_model_predictions_output_test_no_glaciers.csv')


noglaciers = df[['elv_median', 'elv_range', 'area_m2', 'perim_m', 'sl_median',
                'max_annualsum_tp', 'n_rainydays_median', 'precip95', 'cross_zero',
                'frost_days', 'tp', 'snow', 'rain', 'veg_frac', 'target', 'M',
                'circularity_ratio', 'compactness', 'mean_annual_t2m_downsc',
                'cont_permafrost', 'glacier', 'y_result_gl', 'y_result_probs_gl']]



yesglaciers = df_predict_glaciers[['y_result', 'y_result_probs']]


together = noglaciers.join(yesglaciers)


together['diff_in_prob'] = together.y_result_probs - together.y_result_probs_gl


# Plotting the data
plt.figure(figsize=(35, 6))

# Plotting df1 data
sns.barplot(together.index, together['y_result_probs'], label='still glaciers', color='blue')

# # Plotting df2 data
sns.barplot(together.index, together['y_result_probs_gl'], label='no more glaciers', color='red')



# Plotting the data
plt.figure(figsize=(35, 10))

# Plotting df1 data
# sns.barplot(data = together, x = together.index, y= together['diff_in_prob'], hue = together.target)
sns.barplot(data = together, x = together.index, y= together['diff_in_prob'], hue = together.target)









climate = ['elv_median', 'elv_range', 'area_m2', 'perim_m', 'sl_median',
           'max_annualsum_tp', 'n_rainydays_median', 'precip95', 'cross_zero',
           'frost_days', 'tp', 'snow', 'rain', 'veg_frac', 'target', 'M',
           'circularity_ratio', 'compactness', 'mean_annual_t2m_downsc',
           'cont_permafrost', 'glacier']


changed = df_predict[climate]
changed['mean_annual_t2m_downsc'] = changed.mean_annual_t2m_downsc + 2

y = changed.target
X = changed.drop(columns=['target'])


# predictions
model = modelC1[0]

y_result_temp = model.predict(X)
probs_temp = model.predict_proba(X)
probs_temp = probs_temp[:,1]


# # calculate confusions 
# df = changed
changed['y_result_temp'] = y_result_temp
changed['y_result_probs_temp'] = probs_temp



old_temp = df_predict[['y_result', 'y_result_probs', 'target']]
new_temp = changed[['y_result_temp', 'y_result_probs_temp']]

temps = old_temp.join(new_temp)

temps['diff_in_prob'] = temps.y_result_probs - temps.y_result_probs_temp


# Plotting the data
plt.figure(figsize=(35, 6))

# df5 = temps[temps.target == 0]
df5 = temps.sort_values('target').reset_index()

# Plotting df1 data
sns.barplot(df5.index, df5['y_result_probs'], hue=df5.target)

# # Plotting df2 data
sns.barplot(df5.index, df5['y_result_probs_temp'], hue=df5.target, palette=['red', 'green'])

plt.legend()


df5


# Plotting the data
plt.figure(figsize=(35, 10))

# Plotting df1 data
# sns.barplot(data = together, x = together.index, y= together['diff_in_prob'], hue = together.target)
sns.barplot(data = temps, x = temps.index, y= temps['diff_in_prob'], hue = temps.target)



floods = temps[temps.target == 1]

floods['diff_in_prob'].mean()










