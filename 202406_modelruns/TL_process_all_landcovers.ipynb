{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00a46ac3-f921-4728-b92b-df9d01341b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# import xycmap\n",
    "\n",
    "import functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e7676-2b9f-4f86-a2db-67d365b9c451",
   "metadata": {},
   "source": [
    "# sediment yeild as input for SL systems: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52dac53b-dd55-4d41-949a-4ad8db7fcc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caluclate percentiles of TL sediment yeild for 5 different land covers\n",
    "# convert units\n",
    "# assign index location and coordinates\n",
    "# save with_coordinates files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2edfc0fe-7029-4b99-b613-470a5a33c02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.6 s, sys: 780 ms, total: 33.4 s\n",
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ------------\n",
    "# columns: 'Qstl'\n",
    "# landcover = 1, 2, 3, 4, 5\n",
    "# location = 'langtang', 'mustang'\n",
    "# ------------\n",
    "\n",
    "column = 'Qstl'\n",
    "landcover_idx = 5\n",
    "location = 'langtang'\n",
    "\n",
    "# directory_path = f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/TL_data/{landcover_idx}landcover/{location}_climate_cut'\n",
    "# directory_path = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/{landcover_idx}landcover/{location}_climate_cut'\n",
    "\n",
    "directory_path = f'/Volumes/T7 Shield/202409_paper2_modelruns/May2025_30years/TL_data/{landcover_idx}landcover/{location}_climate_cut'\n",
    "\n",
    "mean_annual_sum =[]\n",
    "\n",
    "mean_annual_10 = []\n",
    "mean_annual_20 = []\n",
    "mean_annual_25 = []\n",
    "mean_annual_30 = []\n",
    "mean_annual_40 = []\n",
    "mean_annual_50 = []\n",
    "mean_annual_60 = []\n",
    "mean_annual_70 = []\n",
    "mean_annual_75 = []\n",
    "mean_annual_80 = []\n",
    "mean_annual_90 = []\n",
    "mean_annual_100 = []\n",
    "\n",
    "\n",
    "# Iterate over each folder\n",
    "for folder_name in os.listdir(directory_path):\n",
    "    folder_path = os.path.join(directory_path, folder_name)\n",
    "\n",
    "    if folder_name == '.DS_Store':\n",
    "        continue\n",
    "    if folder_name.startswith('.'):\n",
    "        continue\n",
    "    # skip all csvs\n",
    "    if folder_name.endswith('.csv'):\n",
    "        continue\n",
    "        \n",
    "    # Check if it's a directory and its name starts with 'cellnr'\n",
    "    if os.path.isdir(folder_path): #and folder_name.startswith('cellnr'):\n",
    "\n",
    "        # Locate the 'sediment.out' file within the folder\n",
    "        sediment_out_path = os.path.join(folder_path, 'Sediment.out')\n",
    "        \n",
    "        # Read the contents of the file into a pandas DataFrame\n",
    "        df = pd.read_csv(sediment_out_path, delimiter=',')  # Adjust delimiter if needed\n",
    "        # mean:\n",
    "        mean_annual_sum_value = functions.annual_sum_mean(df)\n",
    "        # percentiles:\n",
    "        mean_annual_10_value = functions.annual_sum_percentile(df, 10)\n",
    "        mean_annual_20_value = functions.annual_sum_percentile(df, 20)\n",
    "        mean_annual_25_value = functions.annual_sum_percentile(df, 25)\n",
    "        mean_annual_30_value = functions.annual_sum_percentile(df, 30)\n",
    "        mean_annual_40_value = functions.annual_sum_percentile(df, 40)\n",
    "        mean_annual_50_value = functions.annual_sum_percentile(df, 50)\n",
    "        mean_annual_60_value = functions.annual_sum_percentile(df, 60)\n",
    "        mean_annual_70_value = functions.annual_sum_percentile(df, 70)\n",
    "        mean_annual_75_value = functions.annual_sum_percentile(df, 75)\n",
    "        mean_annual_80_value = functions.annual_sum_percentile(df, 80)\n",
    "        mean_annual_90_value = functions.annual_sum_percentile(df, 90)\n",
    "        mean_annual_100_value =  functions.annual_sum_percentile(df, 100)\n",
    "\n",
    "       # # Save the DataFrame into the results table\n",
    "        mean_annual_sum.append((folder_name.replace('cellnr', ''), mean_annual_sum_value))\n",
    "\n",
    "        mean_annual_10.append((folder_name.replace('cellnr', ''), mean_annual_10_value ))\n",
    "        mean_annual_20.append((folder_name.replace('cellnr', ''), mean_annual_20_value ))\n",
    "        mean_annual_25.append((folder_name.replace('cellnr', ''), mean_annual_25_value ))\n",
    "        mean_annual_30.append((folder_name.replace('cellnr', ''), mean_annual_30_value ))\n",
    "        mean_annual_40.append((folder_name.replace('cellnr', ''), mean_annual_40_value ))\n",
    "        mean_annual_50.append((folder_name.replace('cellnr', ''), mean_annual_50_value ))\n",
    "        mean_annual_60.append((folder_name.replace('cellnr', ''), mean_annual_60_value ))\n",
    "        mean_annual_70.append((folder_name.replace('cellnr', ''), mean_annual_70_value ))\n",
    "        mean_annual_75.append((folder_name.replace('cellnr', ''), mean_annual_75_value ))\n",
    "        mean_annual_80.append((folder_name.replace('cellnr', ''), mean_annual_80_value ))\n",
    "        mean_annual_90.append((folder_name.replace('cellnr', ''), mean_annual_90_value ))\n",
    "        mean_annual_100.append((folder_name.replace('cellnr', ''),mean_annual_100_value))\n",
    "\n",
    "\n",
    "\n",
    "# make table \n",
    "mean_annual_sum_df = pd.DataFrame(mean_annual_sum, columns=['cellnr', 'annual_mean_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_10_df  = pd.DataFrame(mean_annual_10, columns=['cellnr', 'annual_10percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_20_df  = pd.DataFrame(mean_annual_20, columns=['cellnr', 'annual_20percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_25_df  = pd.DataFrame(mean_annual_25, columns=['cellnr', 'annual_25percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_30_df  = pd.DataFrame(mean_annual_30, columns=['cellnr', 'annual_30percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_40_df  = pd.DataFrame(mean_annual_40, columns=['cellnr', 'annual_40percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_50_df  = pd.DataFrame(mean_annual_50, columns=['cellnr', 'annual_50percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_60_df  = pd.DataFrame(mean_annual_60, columns=['cellnr', 'annual_60percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_70_df  = pd.DataFrame(mean_annual_70, columns=['cellnr', 'annual_70percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_75_df  = pd.DataFrame(mean_annual_75, columns=['cellnr', 'annual_75percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_80_df  = pd.DataFrame(mean_annual_80, columns=['cellnr', 'annual_80percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_90_df  = pd.DataFrame(mean_annual_90, columns=['cellnr', 'annual_90percent_Qstl_mm']).set_index('cellnr') \n",
    "mean_annual_100_df = pd.DataFrame(mean_annual_100, columns=['cellnr', 'annual_100percent_Qstl_mm']).set_index('cellnr')\n",
    "\n",
    "\n",
    "# merge together\n",
    "merged_df = pd.concat([mean_annual_sum_df, mean_annual_10_df, mean_annual_20_df,\n",
    "                       mean_annual_25_df, mean_annual_30_df, mean_annual_40_df, mean_annual_50_df,\n",
    "                       mean_annual_60_df, mean_annual_70_df, mean_annual_75_df, mean_annual_80_df, \n",
    "                       mean_annual_90_df, mean_annual_100_df], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64c09ae3-d74d-430c-bacd-f128964b0849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to m3\n",
    "for column in merged_df.columns:\n",
    "    # for the columns with _mm in it\n",
    "    if '_mm' in column:\n",
    "        # Replace '_mm' with '_m3' and create a new column\n",
    "        new_column_name = column.replace('_mm', '_m3')\n",
    "        # convert to m3: 1) [sediments mm to m] 2) [sediments [m] * area [m2]\n",
    "        merged_df[new_column_name] = (merged_df[column] / 1000) * 4.83 * (10 ** 6)\n",
    "\n",
    "\n",
    "# calculate the volume of m3 per day \n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if '_m3' in column:\n",
    "        merged_df[column + '_day'] = merged_df[column] /365\n",
    "\n",
    "\n",
    "\n",
    "# add coordinates and save the output \n",
    "\n",
    "# csv with coordinates and geopotential (z) \n",
    "\n",
    "coordinates = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/downscaling_simple/coordinates_with_labels_{location}.csv').set_index('cellnr2')\n",
    "with_coords = pd.concat([merged_df, coordinates],  axis=1).reset_index()\n",
    "# coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9ad8c32-3832-4bfd-8a6f-adb0be5ae5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_output'\n",
    "\n",
    "\n",
    "# with_coords.to_csv(output_folder + f'/{location}_{column}_percentiles_for_sediment_input_landcover{landcover_idx}.csv', index = True)\n",
    "\n",
    "\n",
    "with_coords.to_csv(output_folder + f'/{location}_annual_mean_percentiles_for_sediment_input_landcover{landcover_idx}.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f0d907d-d883-4f5e-8ef5-7aee1ac9aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'/{location}_{column}_percentiles_for_sediment_input_landcover{landcover_idx}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebabfcd-4c90-4b45-9cc2-e31eccfcbab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952ecd7-8e52-4944-9d83-e3519999c983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e403b-7e19-4212-885d-d16083b11884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fb2a0a-7397-47c2-9c53-7279e178e6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75614b9-a858-4fff-8c3f-a2ce1120e531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757dbb6-ac17-48a0-9c4c-54b6f34f8a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddccbd81-ef98-4666-abab-692147382c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d9559c1-c8e6-40ab-97c9-a9d26cf9d85a",
   "metadata": {},
   "source": [
    "## calculate annual mean per the 30-years of modelling for all land covers - langtang\n",
    "\n",
    "\n",
    "structure: \n",
    "- 5 different land covers\n",
    "- 3 different regions\n",
    "- this notebook is for langtang "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ed79b5f-9506-465b-bdf1-3b80c57fea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestep workaround\n",
    "column = 'Qstl'\n",
    "landcover_idx = 5\n",
    "location = 'langtang'\n",
    "\n",
    "# folder_path = f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/TL_data/{landcover_idx}landcover/langtang_climate_cut'\n",
    "folder_path = f'/Volumes/T7 Shield/202409_paper2_modelruns/May2025_30years/TL_data/{landcover_idx}landcover/{location}_climate_cut'\n",
    "\n",
    "\n",
    "\n",
    "# read one file to get the time-step\n",
    "df = functions.calculate_monthly_sediment_yield_all(pd.read_csv(folder_path + '/12a/Sediment.out'), column)\n",
    "df = df[['month', 'year']]\n",
    "\n",
    "# elevation langtang\n",
    "elevation = pd.read_csv('/Users/varyabazilova/Desktop/paper2/downscaling_simple/coordinates_and_elevation_with_labels_langtang.csv')[['cellnr2','band_data']] \n",
    "elevation = elevation.transpose()\n",
    "\n",
    "elevation_list = elevation.loc['cellnr2'].tolist()\n",
    "\n",
    "\n",
    "output_folder = '/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b05c863c-e608-4e51-97ff-5d761ded8ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qstl\n",
      "default land cover. monthly data: same\n",
      "landcover_5\n",
      "CPU times: user 16 s, sys: 661 ms, total: 16.7 s\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(column)\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the files in the folders\n",
    "for folder_name in os.listdir(folder_path):\n",
    "    # skip dsstore thing\n",
    "    if folder_name == '.DS_Store':\n",
    "        continue\n",
    "    # skip all csvs\n",
    "    if folder_name.endswith('.csv'):\n",
    "        continue\n",
    "    file_path = os.path.join(folder_path, folder_name, 'Sediment.out')\n",
    "    if os.path.isfile(file_path):\n",
    "        sediments = pd.read_csv(file_path)\n",
    "        # calculate mean monthly value for given column \n",
    "        output_df = functions.calculate_monthly_sediment_yield_all(sediments, column)\n",
    "        # take the column \n",
    "        name_column = output_df[column]\n",
    "        # rename the columns \n",
    "        column_name = f'{column}_{folder_name}'\n",
    "        result_df[column_name] = name_column\n",
    "        result_df.columns = [col[-3:] for col in result_df.columns]\n",
    "\n",
    "# result_df2 = result_df.transpose()\n",
    "\n",
    "result_df = result_df\n",
    "result_df = result_df[elevation_list]\n",
    "\n",
    "if elevation_list == result_df.columns.tolist():\n",
    "    print(\"default land cover. monthly data: same\")\n",
    "else:\n",
    "    print(\"not the same\")\n",
    "\n",
    "\n",
    "# rename columns according the the elevation and merge with timestep \n",
    "result_df.columns = elevation.loc['band_data']\n",
    "result_df = pd.concat([df, result_df],axis=1)\n",
    "result_df['land_cover'] = f'landcover_{landcover_idx}'\n",
    "print(result_df.land_cover[1])\n",
    "\n",
    "\n",
    "result_df.to_csv(output_folder + f'/{location}_monthly_sum_elevation_{column}_landcover{landcover_idx}.csv', index = False)\n",
    "\n",
    "# create a dataframe with timestep and attach it to the monthly data dataframe\n",
    "\n",
    "monthly_data = result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d630847-705c-4db4-974e-2c5311d3125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dad4db-34f6-4884-8fb6-2263f0cf6b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d051c-e3a3-4d89-9fef-e6c461d36817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ab080-3319-43b0-9fdd-a65acc60f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2021d77b-c2b3-46f3-8592-f2a15ab6a11b",
   "metadata": {},
   "source": [
    "# mustang - all months "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41916253-7799-43b8-9219-b543c6414046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestep workaround\n",
    "\n",
    "column = 'Qstl'\n",
    "landcover_idx = 5\n",
    "location = 'mustang'\n",
    "\n",
    "# folder_path = f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/TL_data/{landcover_idx}landcover/{location}_climate_cut'\n",
    "folder_path = f'/Volumes/T7 Shield/202409_paper2_modelruns/May2025_30years/TL_data/{landcover_idx}landcover/{location}_climate_cut'\n",
    "\n",
    "# read one file to get the time-step\n",
    "df = functions.calculate_monthly_sediment_yield_all(pd.read_csv(folder_path + '/13a/Sediment.out'), column)\n",
    "df = df[['month', 'year']]\n",
    "\n",
    "# elevation langtang\n",
    "elevation = pd.read_csv('/Users/varyabazilova/Desktop/paper2/downscaling_simple/coordinates_and_elevation_with_labels_mustang.csv')[['cellnr2','band_data']] \n",
    "elevation = elevation.transpose()\n",
    "\n",
    "elevation_list = elevation.loc['cellnr2'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6d145-99d0-430d-835e-4b72f1c5d002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f381b-f57d-41fa-a609-33e842bd0beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "13ccea92-6a97-41e2-a01e-c352c16890ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qstl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default land cover. monthly data: same\n",
      "landcover_5\n",
      "CPU times: user 1min, sys: 2.98 s, total: 1min 3s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "print(column)\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the files in the folders\n",
    "for folder_name in os.listdir(folder_path):\n",
    "    # skip dsstore thing\n",
    "    if folder_name == '.DS_Store':\n",
    "        continue\n",
    "    # skip all csvs\n",
    "    if folder_name.endswith('.csv'):\n",
    "        continue\n",
    "    file_path = os.path.join(folder_path, folder_name, 'Sediment.out')\n",
    "    if os.path.isfile(file_path):\n",
    "        sediments = pd.read_csv(file_path)\n",
    "        # calculate mean monthly value for given column \n",
    "        output_df = functions.calculate_monthly_sediment_yield_all(sediments, column)\n",
    "        # take the column \n",
    "        name_column = output_df[column]\n",
    "        # rename the columns \n",
    "        column_name = f'{column}_{folder_name}'\n",
    "        result_df[column_name] = name_column\n",
    "        result_df.columns = [col[-3:] for col in result_df.columns]\n",
    "\n",
    "# result_df2 = result_df.transpose()\n",
    "\n",
    "result_df = result_df\n",
    "result_df = result_df[elevation_list]\n",
    "\n",
    "if elevation_list == result_df.columns.tolist():\n",
    "    print(\"default land cover. monthly data: same\")\n",
    "else:\n",
    "    print(\"not the same\")\n",
    "\n",
    "\n",
    "# rename columns according the the elevation and merge with timestep \n",
    "result_df.columns = elevation.loc['band_data']\n",
    "result_df = pd.concat([df, result_df],axis=1)\n",
    "result_df['land_cover'] = f'landcover_{landcover_idx}'\n",
    "print(result_df.land_cover[1])\n",
    "\n",
    "result_df.to_csv(output_folder + f'/{location}_monthly_sum_elevation_{column}_landcover{landcover_idx}.csv', index = False)\n",
    "\n",
    "# create a dataframe with timestep and attach it to the monthly data dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc21c58b-accf-455d-bb7b-567c4947e192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42861bc8-93df-469b-baa2-c20f0ef9a4bf",
   "metadata": {},
   "source": [
    "# calculate mean annual values for all land covers for langtang and mustang areas forthe SL case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f3861e93-8289-4cde-a856-29e5a6ddf38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 4s, sys: 3.49 s, total: 2min 7s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "column = 'Qstl'\n",
    "landcover_idx = 5\n",
    "location = 'mustang'\n",
    "\n",
    "# directory_path = f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/TL_data/{landcover_idx}landcover/{location}_climate_cut'\n",
    "directory_path = f'/Volumes/T7 Shield/202409_paper2_modelruns/May2025_30years/TL_data/{landcover_idx}landcover/{location}_climate_cut'\n",
    "\n",
    "mean_annual_sum =[]\n",
    "\n",
    "mean_annual_10 = []\n",
    "mean_annual_20 = []\n",
    "mean_annual_25 = []\n",
    "mean_annual_30 = []\n",
    "mean_annual_40 = []\n",
    "mean_annual_50 = []\n",
    "mean_annual_60 = []\n",
    "mean_annual_70 = []\n",
    "mean_annual_75 = []\n",
    "mean_annual_80 = []\n",
    "mean_annual_90 = []\n",
    "mean_annual_100 = []\n",
    "\n",
    "\n",
    "# Iterate over each folder\n",
    "for folder_name in os.listdir(directory_path):\n",
    "    folder_path = os.path.join(directory_path, folder_name)\n",
    "\n",
    "    if folder_name == '.DS_Store':\n",
    "        continue\n",
    "    if folder_name.startswith('.'):\n",
    "        continue\n",
    "    # skip all csvs\n",
    "    if folder_name.endswith('.csv'):\n",
    "        continue\n",
    "        \n",
    "    # Check if it's a directory and its name starts with 'cellnr'\n",
    "    if os.path.isdir(folder_path): #and folder_name.startswith('cellnr'):\n",
    "\n",
    "        # Locate the 'sediment.out' file within the folder\n",
    "        sediment_out_path = os.path.join(folder_path, 'Sediment.out')\n",
    "        \n",
    "        # Read the contents of the file into a pandas DataFrame\n",
    "        df = pd.read_csv(sediment_out_path, delimiter=',')  # Adjust delimiter if needed\n",
    "        # mean:\n",
    "        mean_annual_sum_value = functions.annual_sum_mean(df)\n",
    "        # percentiles:\n",
    "        mean_annual_10_value = functions.annual_sum_percentile(df, 10)\n",
    "        mean_annual_20_value = functions.annual_sum_percentile(df, 20)\n",
    "        mean_annual_25_value = functions.annual_sum_percentile(df, 25)\n",
    "        mean_annual_30_value = functions.annual_sum_percentile(df, 30)\n",
    "        mean_annual_40_value = functions.annual_sum_percentile(df, 40)\n",
    "        mean_annual_50_value = functions.annual_sum_percentile(df, 50)\n",
    "        mean_annual_60_value = functions.annual_sum_percentile(df, 60)\n",
    "        mean_annual_70_value = functions.annual_sum_percentile(df, 70)\n",
    "        mean_annual_75_value = functions.annual_sum_percentile(df, 75)\n",
    "        mean_annual_80_value = functions.annual_sum_percentile(df, 80)\n",
    "        mean_annual_90_value = functions.annual_sum_percentile(df, 90)\n",
    "        mean_annual_100_value =  functions.annual_sum_percentile(df, 100)\n",
    "\n",
    "       # # Save the DataFrame into the results table\n",
    "        mean_annual_sum.append((folder_name.replace('cellnr', ''), mean_annual_sum_value))\n",
    "\n",
    "        mean_annual_10.append((folder_name.replace('cellnr', ''), mean_annual_10_value ))\n",
    "        mean_annual_20.append((folder_name.replace('cellnr', ''), mean_annual_20_value ))\n",
    "        mean_annual_25.append((folder_name.replace('cellnr', ''), mean_annual_25_value ))\n",
    "        mean_annual_30.append((folder_name.replace('cellnr', ''), mean_annual_30_value ))\n",
    "        mean_annual_40.append((folder_name.replace('cellnr', ''), mean_annual_40_value ))\n",
    "        mean_annual_50.append((folder_name.replace('cellnr', ''), mean_annual_50_value ))\n",
    "        mean_annual_60.append((folder_name.replace('cellnr', ''), mean_annual_60_value ))\n",
    "        mean_annual_70.append((folder_name.replace('cellnr', ''), mean_annual_70_value ))\n",
    "        mean_annual_75.append((folder_name.replace('cellnr', ''), mean_annual_75_value ))\n",
    "        mean_annual_80.append((folder_name.replace('cellnr', ''), mean_annual_80_value ))\n",
    "        mean_annual_90.append((folder_name.replace('cellnr', ''), mean_annual_90_value ))\n",
    "        mean_annual_100.append((folder_name.replace('cellnr', ''),mean_annual_100_value))\n",
    "\n",
    "\n",
    "\n",
    "# make table \n",
    "mean_annual_sum_df = pd.DataFrame(mean_annual_sum, columns=['cellnr', 'annual_mean_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_10_df  = pd.DataFrame(mean_annual_10, columns=['cellnr', 'annual_10percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_20_df  = pd.DataFrame(mean_annual_20, columns=['cellnr', 'annual_20percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_25_df  = pd.DataFrame(mean_annual_25, columns=['cellnr', 'annual_25percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_30_df  = pd.DataFrame(mean_annual_30, columns=['cellnr', 'annual_30percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_40_df  = pd.DataFrame(mean_annual_40, columns=['cellnr', 'annual_40percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_50_df  = pd.DataFrame(mean_annual_50, columns=['cellnr', 'annual_50percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_60_df  = pd.DataFrame(mean_annual_60, columns=['cellnr', 'annual_60percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_70_df  = pd.DataFrame(mean_annual_70, columns=['cellnr', 'annual_70percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_75_df  = pd.DataFrame(mean_annual_75, columns=['cellnr', 'annual_75percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_80_df  = pd.DataFrame(mean_annual_80, columns=['cellnr', 'annual_80percent_Qstl_mm']).set_index('cellnr')\n",
    "mean_annual_90_df  = pd.DataFrame(mean_annual_90, columns=['cellnr', 'annual_90percent_Qstl_mm']).set_index('cellnr') \n",
    "mean_annual_100_df = pd.DataFrame(mean_annual_100, columns=['cellnr', 'annual_100percent_Qstl_mm']).set_index('cellnr')\n",
    "\n",
    "\n",
    "# merge together\n",
    "merged_df = pd.concat([mean_annual_sum_df, mean_annual_10_df, mean_annual_20_df,\n",
    "                       mean_annual_25_df, mean_annual_30_df, mean_annual_40_df, mean_annual_50_df,\n",
    "                       mean_annual_60_df, mean_annual_70_df, mean_annual_75_df, mean_annual_80_df, \n",
    "                       mean_annual_90_df, mean_annual_100_df], axis = 1)\n",
    "\n",
    "# monthly_mean.to_csv(output_folder + f'/{location}_long_term_mean_monthly_{column}_elevation_landcover{landcover_idx}.csv', index = False)\n",
    "\n",
    "# merged_df.to_csv(output_folder + f'/{location}_annual_mean_percentiles_for_sediment_input_landcover{landcover_idx}.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "13af130b-1bad-4433-82c9-0c577fb623a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to m3\n",
    "for column in merged_df.columns:\n",
    "    # for the columns with _mm in it\n",
    "    if '_mm' in column:\n",
    "        # Replace '_mm' with '_m3' and create a new column\n",
    "        new_column_name = column.replace('_mm', '_m3')\n",
    "        # convert to m3: 1) [sediments mm to m] 2) [sediments [m] * area [m2]\n",
    "        merged_df[new_column_name] = (merged_df[column] / 1000) * 4.83 * (10 ** 6)\n",
    "\n",
    "\n",
    "# calculate the volume of m3 per day \n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if '_m3' in column:\n",
    "        merged_df[column + '_day'] = merged_df[column] /365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71f83af4-5587-4c7c-a34c-af434e5fc9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add coordinates and save the output \n",
    "\n",
    "# csv with coordinates and geopotential (z) \n",
    "\n",
    "coordinates = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/downscaling_simple/coordinates_with_labels_{location}.csv').set_index('cellnr2')\n",
    "with_coords = pd.concat([merged_df, coordinates],  axis=1).reset_index()\n",
    "# coordinates\n",
    "\n",
    "with_coords.to_csv(output_folder + f'/{location}_annual_mean_percentiles_for_sediment_input_landcover{landcover_idx}.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52dca0-95f8-405f-af2f-792784f4cbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31fcdd3-0945-4340-8fee-7424429ef69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6b009-e4b2-4b95-8761-8b0e971c778c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754b453-08d4-4154-b425-452249ef3c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88ca03-f45e-4f10-9cd1-78a5660a173c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
