# MARCH 2025 - calculate floodd and potential DFs for mustang to make a bubble plot 


# Filter the DataFrame for the specified date range


start_date = "1990-08-31"
end_date = "2021-06-30"


def filter_by_date(df, start_date = start_date, end_date = end_date, date_column='D'):
    """
    Filters a DataFrame to include only rows where the date_column is within the given range.

    Parameters:
        df (pd.DataFrame): The input DataFrame.
        start_date (str or pd.Timestamp): The start date (inclusive).
        end_date (str or pd.Timestamp): The end date (inclusive).
        date_column (str): The column containing date values (default is 'D').

    Returns:
        pd.DataFrame: The filtered DataFrame.
    """
    df[date_column] = pd.to_datetime(df[date_column])  # Convert to datetime
    return df[(df[date_column] >= start_date) & (df[date_column] <= end_date)]



'''

def prepare_dfcount(dfcount):
    dfcount['D'] = pd.to_datetime(dfcount['D'])
    
    dfcount = dfcount.drop('D', axis=1)
    dfcount = dfcount.rename(columns={'D_year': 'year', 'D_month': 'month'})
    
    # Melt the dataframe
    melted = pd.melt(dfcount, id_vars=['year', 'month'], var_name='elevation', value_name='dfs_count')
    
    # Process elevation column
    melted['elevation'] = melted['elevation'].str.split('.').str[0].astype(float)

df[['base', 'suffix']] = df['elevation'].astype(str).str.extract(r'(\d+)(?:\.\d+)?(?:\.(\d+))?')
df['base'] = df['base'].astype(int)
df['suffix'] = df['suffix'].fillna(0).astype(int)
df['elevation'] = df['base'] + df['suffix']

    
    melted['elevation_bin'] = melted.apply(functions.bin_elevation500, axis=1)
    
    # Create unique id for elevation and month for merging
    melted['elevation'] = melted['elevation'].astype(int)
    melted['id'] = melted.index.astype(str) + "_" + melted['elevation'].astype(str) + "_" + melted['year'].astype(str) + "_" + melted['month'].astype(str)
    # melted['id'] = melted['elevation'].astype(str) + "_" + melted['year'].astype(str) + "_" + melted['month'].astype(str)
    
    return melted
'''

def prepare_dfcount(dfcount):
    dfcount['D'] = pd.to_datetime(dfcount['D'])
    
    dfcount = dfcount.drop('D', axis=1)
    dfcount = dfcount.rename(columns={'D_year': 'year', 'D_month': 'month'})
    
    # Melt the dataframe
    melted = pd.melt(dfcount, id_vars=['year', 'month'], var_name='elevation', value_name='dfs_count')
    
    # Process elevation column
    melted[['base', 'suffix']] = melted['elevation'].astype(str).str.extract(r'(\d+)(?:\.\d+)?(?:\.(\d+))?')
    melted['base'] = melted['base'].astype(int)
    melted['suffix'] = melted['suffix'].fillna(0).astype(int)
    melted['elevation'] = melted['base'] + melted['suffix']*10
    melted['elevation'] = melted['elevation'].astype(float)


    
    melted['elevation_bin'] = melted.apply(functions.bin_elevation500, axis=1)
    
    # Create unique id for elevation and month for merging
    melted['elevation'] = melted['elevation'].astype(int)
    # melted['id'] = melted.index.astype(str) + "_" + melted['elevation'].astype(str) + "_" + melted['year'].astype(str) + "_" + melted['month'].astype(str)
    melted['id'] = melted['elevation'].astype(str) + "_" + melted['year'].astype(str) + "_" + melted['month'].astype(str)
    
    return melted







def calculate_floods(df):
    # df percent 
    df[f'dfs_count_60_percent'] = (df[f'dfs_count_60percent'] *100) / df.dfspot_count
    df[f'dfs_count_50_percent'] = (df[f'dfs_count_50percent'] *100) / df.dfspot_count
    df[f'dfs_count_40_percent'] = (df[f'dfs_count_40percent'] *100) / df.dfspot_count
    df[f'dfs_count_30_percent'] = (df[f'dfs_count_30percent'] *100) / df.dfspot_count
    df[f'dfs_count_20_percent'] = (df[f'dfs_count_20percent'] *100) / df.dfspot_count
    # ff count 
    df[f'ffs_count_60'] = df.dfspot_count - df[f'dfs_count_60percent']
    df[f'ffs_count_50'] = df.dfspot_count - df[f'dfs_count_50percent']
    df[f'ffs_count_40'] = df.dfspot_count - df[f'dfs_count_40percent']
    df[f'ffs_count_30'] = df.dfspot_count - df[f'dfs_count_30percent']
    df[f'ffs_count_20'] = df.dfspot_count - df[f'dfs_count_20percent']
    # ff percent
    df[f'ffs_count_60_percent'] = (df[f'ffs_count_60'] *100) / df.dfspot_count
    df[f'ffs_count_50_percent'] = (df[f'ffs_count_50'] *100) / df.dfspot_count
    df[f'ffs_count_40_percent'] = (df[f'ffs_count_40'] *100) / df.dfspot_count
    df[f'ffs_count_30_percent'] = (df[f'ffs_count_30'] *100) / df.dfspot_count
    df[f'ffs_count_20_percent'] = (df[f'ffs_count_20'] *100) / df.dfspot_count
    return df



def merge_landcover_dfs(dfs, dfspot, merge_on, dfspot_column):
    """
    Merges a list of dataframes on specified columns and adds a column from a 'dfspot' dataframe.
    
    Parameters:
        dfs (list of pd.DataFrame): List of dataframes to merge sequentially.
        dfspot (pd.DataFrame): A dataframe containing a column to add after the merges.
        merge_on (list of str): Columns to use as the merge keys.
        dfspot_column (str): The name of the column in `dfspot` to add after merging.
        
    Returns:
        pd.DataFrame: The merged dataframe with the additional column from `dfspot`.
    """
    
    # Start with the first dataframe in `dfs` and automatically get the count column
    count_column = [col for col in dfs[0].columns if col.startswith('dfs_count')][0]
    merged_df = dfs[0][merge_on + [count_column]]
    
    # Iterate over remaining dataframes and merge each sequentially
    for df in dfs[1:]:
        count_column = [col for col in df.columns if col.startswith('dfs_count')][0]
        merged_df = pd.merge(merged_df, df[merge_on + [count_column]], on=merge_on)
    
    # Add the `dfspot_column` from `dfspot` dataframe to the merged dataframe
    merged_df['dfspot_count'] = dfspot[dfspot_column]
    
    return merged_df



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import glob

import functions


# 1 dfspot
location = 'mustang'

path = '/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025May_output/dfspot/'
# path = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025Jan_output/TL/dfspot_count/{location}/'

dfspot1_df = pd.read_csv(path + f'{location}_monthly_dfspot_count_landcover1.csv', index_col = 0).fillna(0)
dfspot2_df = pd.read_csv(path + f'{location}_monthly_dfspot_count_landcover2.csv', index_col = 0).fillna(0)
dfspot3_df = pd.read_csv(path + f'{location}_monthly_dfspot_count_landcover3.csv', index_col = 0).fillna(0)
dfspot4_df = pd.read_csv(path + f'{location}_monthly_dfspot_count_landcover4.csv', index_col = 0).fillna(0)
dfspot5_df = pd.read_csv(path + f'{location}_monthly_dfspot_count_landcover5.csv', index_col = 0).fillna(0)


# dfspot1_df = filter_by_date(dfspot1_df)
# dfspot2_df = filter_by_date(dfspot2_df)
# dfspot3_df = filter_by_date(dfspot3_df)
# dfspot4_df = filter_by_date(dfspot4_df)
# dfspot5_df = filter_by_date(dfspot5_df)

dfspot1 = prepare_dfcount(dfspot1_df).fillna(0)
dfspot2 = prepare_dfcount(dfspot2_df).fillna(0)
dfspot3 = prepare_dfcount(dfspot3_df).fillna(0)
dfspot4 = prepare_dfcount(dfspot4_df).fillna(0)
dfspot5 = prepare_dfcount(dfspot5_df).fillna(0)


# dfspot1['landcover'] = 'langcover 1'
# dfspot2['landcover'] = 'langcover 2'
# dfspot3['landcover'] = 'langcover 3'
# dfspot4['landcover'] = 'langcover 4'
# dfspot5['landcover'] = 'langcover 5'




# dfspot5





























dfspot1_df.drop(['D', 'D_year', 'D_month'], axis = 1).plot(legend = False)



# # Base directory where all output_Xpercent folders are located
# base_directory = "/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025Jan_output/SL_daily/"

# # Use glob to find all relevant files in output_XXpercent folders
# file_pattern = f"{base_directory}/output_*/mustang_monthly_dfs_count_*_landcover{landcover_idx}.csv"
# files = glob.glob(file_pattern)
# print(files)

print(location)


import glob
import pandas as pd
import re

landcover_idx = 5
method = 'once'



# Base directory where all output_Xpercent folders are located
base_directory = f"/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025May_output/SL_{method}/"

# Use glob to find all relevant files in output_XXpercent folders
file_pattern = f"{base_directory}/output_*/{location}_monthly_dfs_count_*_landcover{landcover_idx}.csv"
files = glob.glob(file_pattern)

dfs = []

# Loop through the files and process them
for file in files:
    # Extract the percentile from the folder name
    percentile = re.search(r'output_(\d+percent)', file).group(1)
    
    # Extract the landcover number from the file name
    landcover_number = re.search(r'landcover(\d+)', file).group(1)
    print(landcover_number)
    print(file)
    # Read the file into a DataFrame
    df = pd.read_csv(file, index_col=0).fillna(0)


    # df = filter_by_date(df)

    # Apply your processing function
    processed_df = prepare_dfcount(df)
    
    # Rename the 'dfs_count' column to include percentile and landcover number
    processed_df = processed_df.rename(columns={'dfs_count': f'dfs_count_{percentile}'})
    # Append to the list
    dfs.append(processed_df)

merge_on = ['year', 'month', 'elevation', 'elevation_bin', 'id']
# merge_on = ['id']
dfspot_column = 'dfs_count'

if landcover_idx == 1:
    dfspot = dfspot1
if landcover_idx == 2:
    dfspot = dfspot2
if landcover_idx == 3:
    dfspot = dfspot3
if landcover_idx == 4:
    dfspot = dfspot4
if landcover_idx == 5:
    dfspot = dfspot5


dfmerged = merge_landcover_dfs(dfs, dfspot, merge_on, dfspot_column)
dffloods = calculate_floods(dfmerged)


outpath = '/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025May_output/df_vs_floods/'
dffloods.to_csv(outpath + f'{location}_df_vs_floods_{method}_landcover{landcover_idx}.csv')















# test

df = pd.read_csv(outpath + f'{location}_df_vs_floods_{method}_landcover{landcover_idx}.csv')



df.ffs_count_50_percent.plot()















