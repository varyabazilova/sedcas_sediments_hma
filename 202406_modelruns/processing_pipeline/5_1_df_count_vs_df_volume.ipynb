{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf70767-bbb3-4afc-be1d-ecff6ac85211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARCH 2025 - calculate floodd and potential DFs for mustang to make a bubble plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4ad7d50d-c621-47d7-a12f-05e5741d67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for the specified date range\n",
    "\n",
    "\n",
    "start_date = \"1990-08-31\"\n",
    "end_date = \"2021-06-30\"\n",
    "\n",
    "\n",
    "def filter_by_date(df, start_date = start_date, end_date = end_date, date_column='D'):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame to include only rows where the date_column is within the given range.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        start_date (str or pd.Timestamp): The start date (inclusive).\n",
    "        end_date (str or pd.Timestamp): The end date (inclusive).\n",
    "        date_column (str): The column containing date values (default is 'D').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column])  # Convert to datetime\n",
    "    return df[(df[date_column] >= start_date) & (df[date_column] <= end_date)]\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def prepare_dfcount(dfcount):\n",
    "    dfcount['D'] = pd.to_datetime(dfcount['D'])\n",
    "    \n",
    "    dfcount = dfcount.drop('D', axis=1)\n",
    "    dfcount = dfcount.rename(columns={'D_year': 'year', 'D_month': 'month'})\n",
    "    \n",
    "    # Melt the dataframe\n",
    "    melted = pd.melt(dfcount, id_vars=['year', 'month'], var_name='elevation', value_name='dfs_count')\n",
    "    \n",
    "    # Process elevation column\n",
    "    melted['elevation'] = melted['elevation'].str.split('.').str[0].astype(float)\n",
    "\n",
    "df[['base', 'suffix']] = df['elevation'].astype(str).str.extract(r'(\\d+)(?:\\.\\d+)?(?:\\.(\\d+))?')\n",
    "df['base'] = df['base'].astype(int)\n",
    "df['suffix'] = df['suffix'].fillna(0).astype(int)\n",
    "df['elevation'] = df['base'] + df['suffix']\n",
    "\n",
    "    \n",
    "    melted['elevation_bin'] = melted.apply(functions.bin_elevation500, axis=1)\n",
    "    \n",
    "    # Create unique id for elevation and month for merging\n",
    "    melted['elevation'] = melted['elevation'].astype(int)\n",
    "    melted['id'] = melted.index.astype(str) + \"_\" + melted['elevation'].astype(str) + \"_\" + melted['year'].astype(str) + \"_\" + melted['month'].astype(str)\n",
    "    # melted['id'] = melted['elevation'].astype(str) + \"_\" + melted['year'].astype(str) + \"_\" + melted['month'].astype(str)\n",
    "    \n",
    "    return melted\n",
    "'''\n",
    "\n",
    "def prepare_dfcount(dfcount):\n",
    "    # dfcount['D'] = pd.to_datetime(dfcount['D'])\n",
    "    \n",
    "    # dfcount = dfcount.drop('D', axis=1)\n",
    "    dfcount = dfcount.drop('land_cover', axis = 1)\n",
    "    \n",
    "    # dfcount = dfcount.rename(columns={'D_year': 'year', 'D_month': 'month'})\n",
    "    \n",
    "    # Melt the dataframe\n",
    "    melted = pd.melt(dfcount, id_vars=['year', 'month'], var_name='elevation', value_name='dfs_volume')\n",
    "    \n",
    "    # Process elevation column\n",
    "    melted[['base', 'suffix']] = melted['elevation'].astype(str).str.extract(r'(\\d+)(?:\\.\\d+)?(?:\\.(\\d+))?')\n",
    "    melted['base'] = melted['base'].astype(int)\n",
    "    melted['suffix'] = melted['suffix'].fillna(0).astype(int)\n",
    "    melted['elevation'] = melted['base'] + melted['suffix']*10\n",
    "    melted['elevation'] = melted['elevation'].astype(float)\n",
    "\n",
    "\n",
    "    \n",
    "    melted['elevation_bin'] = melted.apply(functions.bin_elevation500, axis=1)\n",
    "    \n",
    "    # Create unique id for elevation and month for merging\n",
    "    melted['elevation'] = melted['elevation'].astype(int)\n",
    "    # melted['id'] = melted.index.astype(str) + \"_\" + melted['elevation'].astype(str) + \"_\" + melted['year'].astype(str) + \"_\" + melted['month'].astype(str)\n",
    "    melted['id'] = melted['elevation'].astype(str) + \"_\" + melted['year'].astype(str) + \"_\" + melted['month'].astype(str)\n",
    "    \n",
    "    return melted\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dfvol_limitation(df_vol):\n",
    "    \n",
    "    df_vol = df_vol.drop('folder', axis=1)\n",
    "    melted = pd.melt(df_vol, id_vars=['year', 'month'], var_name='elevation', value_name='dfs_volume')\n",
    "    melted[['base', 'suffix']] = melted['elevation'].astype(str).str.extract(r'(\\d+)(?:\\.\\d+)?(?:\\.(\\d+))?')\n",
    "    melted['base'] = melted['base'].astype(int)\n",
    "    melted['suffix'] = melted['suffix'].fillna(0).astype(int)\n",
    "    melted['elevation'] = melted['base'] + melted['suffix']*10\n",
    "    melted['elevation'] = melted['elevation'].astype(float)\n",
    "    melted['elevation_bin'] = melted.apply(functions.bin_elevation500, axis=1)\n",
    "    # Create unique id for elevation and month for merging\n",
    "    melted['elevation'] = melted['elevation'].astype(int)\n",
    "    melted['id'] = melted['elevation'].astype(str) + \"_\" + melted['year'].astype(str) + \"_\" + melted['month'].astype(str)\n",
    "    \n",
    "    return melted\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def merge_landcover_dfs(dfs, dfspot, merge_on, dfspot_column):\n",
    "#     \"\"\"\n",
    "#     Merges a list of dataframes on specified columns and adds a column from a 'dfspot' dataframe.\n",
    "    \n",
    "#     Parameters:\n",
    "#         dfs (list of pd.DataFrame): List of dataframes to merge sequentially.\n",
    "#         dfspot (pd.DataFrame): A dataframe containing a column to add after the merges.\n",
    "#         merge_on (list of str): Columns to use as the merge keys.\n",
    "#         dfspot_column (str): The name of the column in `dfspot` to add after merging.\n",
    "        \n",
    "#     Returns:\n",
    "#         pd.DataFrame: The merged dataframe with the additional column from `dfspot`.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Start with the first dataframe in `dfs` and automatically get the count column\n",
    "#     count_column = [col for col in dfs[0].columns if col.startswith('dfs_volume')][0]\n",
    "#     merged_df = dfs[0][merge_on + [count_column]]\n",
    "    \n",
    "#     # Iterate over remaining dataframes and merge each sequentially\n",
    "#     for df in dfs[1:]:\n",
    "#         count_column = [col for col in df.columns if col.startswith('dfs_volume')][0]\n",
    "#         merged_df = pd.merge(merged_df, df[merge_on + [count_column]], on=merge_on)\n",
    "    \n",
    "#     # Add the `dfspot_column` from `dfspot` dataframe to the merged dataframe\n",
    "#     merged_df['dfspot_volume'] = dfspot[dfspot_column]\n",
    "    \n",
    "#     return merged_df\n",
    "\n",
    "\n",
    "\n",
    "def merge_landcover_dfs(dfs, merge_on):\n",
    "    \"\"\"\n",
    "    Merges a list of dataframes on specified columns and adds a column from a 'dfspot' dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        dfs (list of pd.DataFrame): List of dataframes to merge sequentially.\n",
    "        dfspot (pd.DataFrame): A dataframe containing a column to add after the merges.\n",
    "        merge_on (list of str): Columns to use as the merge keys.\n",
    "        dfspot_column (str): The name of the column in `dfspot` to add after merging.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The merged dataframe with the additional column from `dfspot`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with the first dataframe in `dfs` and automatically get the count column\n",
    "    count_column = [col for col in dfs[0].columns if col.startswith('dfs_volume')][0]\n",
    "    merged_df = dfs[0][merge_on + [count_column]]\n",
    "    \n",
    "    # Iterate over remaining dataframes and merge each sequentially\n",
    "    for df in dfs[1:]:\n",
    "        count_column = [col for col in df.columns if col.startswith('dfs_volume')][0]\n",
    "        merged_df = pd.merge(merged_df, df[merge_on + [count_column]], on=merge_on)\n",
    "    \n",
    "    # Add the `dfspot_column` from `dfspot` dataframe to the merged dataframe\n",
    "    # merged_df['dfspot_volume'] = dfspot[dfspot_column]\n",
    "    \n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f4c5c273-de6f-487c-9690-0050a0703f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a0bb9-2093-402b-96d2-6a69fcf52590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ecbf99-416a-48a1-b5bf-738c458f3da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd33c7-be45-46b4-890b-68be52484a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f21e24c2-cf3f-47bb-b7eb-fd4a3e505a0c",
   "metadata": {},
   "source": [
    "# QSTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164aaab-bfe7-4258-b575-2c00e102c4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c58a5-b53c-4e36-b360-66608aa84ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc137f-db87-449d-b461-309b559e5a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd8483d-5b49-414a-8fd4-50cdb59386c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e28a693a-09b6-4311-96da-ffdc87e22deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Base directory where all output_Xpercent folders are located\n",
    "# base_directory = \"/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025Jan_output/SL_daily/\"\n",
    "\n",
    "# # Use glob to find all relevant files in output_XXpercent folders\n",
    "# file_pattern = f\"{base_directory}/output_*/mustang_monthly_dfs_count_*_landcover{landcover_idx}.csv\"\n",
    "# files = glob.glob(file_pattern)\n",
    "# print(files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a6f08d4e-52c6-458d-8317-b6b8e41c5fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025May_output/SL_once/output_30percent/mustang_monthly_sum_elevation_dfs_30percent_5landcover_mm.csv\n",
      "5\n",
      "/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025May_output/SL_once/output_20percent/mustang_monthly_sum_elevation_dfs_20percent_5landcover_mm.csv\n",
      "5\n",
      "/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025May_output/SL_once/output_60percent/mustang_monthly_sum_elevation_dfs_60percent_5landcover_mm.csv\n",
      "5\n",
      "/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025May_output/SL_once/output_40percent/mustang_monthly_sum_elevation_dfs_40percent_5landcover_mm.csv\n",
      "5\n",
      "/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025May_output/SL_once/output_50percent/mustang_monthly_sum_elevation_dfs_50percent_5landcover_mm.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "location = 'mustang'\n",
    "landcover_idx = 5\n",
    "method = 'once'\n",
    "\n",
    "\n",
    "# Base directory where all output_Xpercent folders are located\n",
    "base_directory = f\"/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025May_output/SL_{method}\"\n",
    "\n",
    "# Use glob to find all relevant files in output_XXpercent folders\n",
    "file_pattern = f\"{base_directory}/output_*/{location}_monthly_sum_elevation_dfs_*_{landcover_idx}landcover_mm.csv\"\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# print(file_pattern)\n",
    "# print(files)\n",
    "\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# Loop through the files and process them\n",
    "for file in files:\n",
    "    # Extract the percentile from the folder name\n",
    "    percentile = re.search(r'output_(\\d+percent)', file).group(1)\n",
    "    \n",
    "    # Extract the landcover number from the file name\n",
    "    landcover_number = re.search(r'(\\d+)landcover', file).group(1)\n",
    "    print(landcover_number)\n",
    "    print(file)\n",
    "    # Read the file into a DataFrame\n",
    "    df = pd.read_csv(file).fillna(0)\n",
    "\n",
    "\n",
    "    # df = filter_by_date(df)\n",
    "\n",
    "    # Apply your processing function\n",
    "    processed_df = prepare_dfvol_limitation(df)\n",
    "    \n",
    "    # Rename the 'dfs_count' column to include percentile and landcover number\n",
    "    processed_df = processed_df.rename(columns={'dfs_volume': f'dfs_volume_{percentile}'})\n",
    "\n",
    "\n",
    "    \n",
    "    # Append to the list\n",
    "    dfs.append(processed_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merge_on = ['year', 'month', 'elevation', 'elevation_bin', 'id']\n",
    "# merge_on = ['id']\n",
    "dfspot_column = 'dfs_volume'\n",
    "\n",
    "if landcover_idx == 1:\n",
    "    dfspot = dfspot1\n",
    "if landcover_idx == 2:\n",
    "    dfspot = dfspot2\n",
    "if landcover_idx == 3:\n",
    "    dfspot = dfspot3\n",
    "if landcover_idx == 4:\n",
    "    dfspot = dfspot4\n",
    "if landcover_idx == 5:\n",
    "    dfspot = dfspot5\n",
    "\n",
    "\n",
    "dfmerged = merge_landcover_dfs(dfs, merge_on)\n",
    "# dffloods = calculate_floods(dfmerged)\n",
    "\n",
    "\n",
    "outpath = '/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025May_output/df_vs_floods/dfs_volume/'\n",
    "dfmerged.to_csv(outpath + f'{location}_df_volumes_{method}_landcover{landcover_idx}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc2350-af17-4617-b200-3f75941e6675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c69f30-fca3-4209-8885-2adb2cede872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a969a82-3845-4c52-bcb8-33608df12aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41da36-8703-4ab5-ae3e-f354063107dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad884c-81e9-4f64-a9ea-0ea0ab0a9886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dac937-9112-4785-9bbd-505de3e75512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43af45-9bb5-4a98-9ef9-292462429694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee739aec-a8b7-46d2-9593-d02a525b0b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b3c1a6-4856-44f1-b5ed-698cd34a6d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4820da32-e94e-4368-848b-03b0b72b2868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900e036-461e-4864-898e-6e3d0998ed35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db4e81-212d-4dfe-afc3-9ccde5656bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364b583-a17e-4c92-aa1e-12c08a3489db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a9c41cdd-7f5d-4cc9-b9f7-d2917f678769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025May_output/SL_daily/output_20percent/langtang_monthly_sum_elevation_dfs_20percent_1landcover_mm.csv')\n",
    "\n",
    "test = pd.read_csv('/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025May_output/SL_once/output_30percent/langtang_monthly_sum_elevation_dfs_30percent_1landcover_mm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e84beb4a-55f3-40cd-8cda-8a76dac748c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c788d058-c4e6-46c8-9fc5-cf0d5642d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop('folder', axis=1)\n",
    "melted = pd.melt(test, id_vars=['year', 'month'], var_name='elevation', value_name='dfs_volume')\n",
    "\n",
    "melted[['base', 'suffix']] = melted['elevation'].astype(str).str.extract(r'(\\d+)(?:\\.\\d+)?(?:\\.(\\d+))?')\n",
    "melted['base'] = melted['base'].astype(int)\n",
    "melted['suffix'] = melted['suffix'].fillna(0).astype(int)\n",
    "melted['elevation'] = melted['base'] + melted['suffix']*10\n",
    "melted['elevation'] = melted['elevation'].astype(float)\n",
    "\n",
    "\n",
    "melted['elevation_bin'] = melted.apply(functions.bin_elevation500, axis=1)\n",
    "    \n",
    "# Create unique id for elevation and month for merging\n",
    "melted['elevation'] = melted['elevation'].astype(int)\n",
    "melted['id'] = melted['elevation'].astype(str) + \"_\" + melted['year'].astype(str) + \"_\" + melted['month'].astype(str)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e0bb4d-317f-4106-ab68-e48117185a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sedcas] *",
   "language": "python",
   "name": "conda-env-sedcas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
