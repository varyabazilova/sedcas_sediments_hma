{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c97203-339e-4ed1-af44-f7f25077ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan\n",
    "\n",
    "# - caclulate monthly Q for different land covers \n",
    "# - calculate how on average change in lang cover affect the change in runoff -> transport capacity -> TL case \n",
    "\n",
    "\n",
    "\n",
    "# plan\n",
    "# 1 - calculate the monthly water yeild \n",
    "# 2 - plot average + spread per month \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb11dc-35d7-40c2-be77-03878577e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db16da3-50dd-4ccd-9d4f-31b945762670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydrology does not depend on the sediment input, so in principle hydro.out is the same all the time and only differs per land cover \n",
    "# -> its ok to take it from the (chosen) sediment input\n",
    "# NB: SO FAR (as of 9/12/2024) the landcover 5 has only been run on the TL scenario and is stored in a different place \n",
    "# to compare the land covers \n",
    "testfile = pd.read_csv('/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/SL_data/1landcover/langtang_climate_cut/12a/Hydro.out')\n",
    "testfile.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d6ba4-5908-43ed-9e02-4ca78d89058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('glacier melt per year roughly:', testfile.glacier_melt.sum()/30)\n",
    "print('Pr per year roughly:',testfile.Pr.sum()/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c4d9a-77cf-498d-8e73-4c6260687ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97b25c-b34b-48d9-a933-240047dd7b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testfile.Q.plot()\n",
    "\n",
    "testfile.snowacc.plot()\n",
    "testfile.glacier_melt.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232cec3b-87ce-4ed2-88a0-58ef5a5c73e3",
   "metadata": {},
   "source": [
    "# discharge in m3, but what about other water components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1055a-9b57-470b-b78c-01e9f4cbc63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def calculate_monthly_water_volume_all(hydro, column, area=4830.0):\n",
    "    \"\"\"\n",
    "    Calculate the sum of water or sediment volume per month for a specific column, \n",
    "    and return the mean volumes across years.\n",
    "    \n",
    "    Parameters:\n",
    "    hydro (pd.DataFrame): Input DataFrame with columns `D` (date) and the specified numeric column.\n",
    "    column (str): The name of the column to calculate the volume for.\n",
    "    area (float): Area in square meters used to scale the volumes. Default is 4830.0.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with the calculated monthly volumes.\n",
    "    \"\"\"\n",
    "    # Ensure `D` column is in datetime format\n",
    "    hydro['D'] = pd.to_datetime(hydro['D'])\n",
    "    \n",
    "    # Check if the specified column exists\n",
    "    if column not in hydro.columns:\n",
    "        raise ValueError(f\"The specified column '{column}' does not exist in the DataFrame.\")\n",
    "    \n",
    "    # Scale the specified column by the area\n",
    "    hydro_scaled = hydro.copy()\n",
    "    hydro_scaled[column] = hydro_scaled[column] * area\n",
    "    \n",
    "    # Set 'D' as the index\n",
    "    hydro_scaled = hydro_scaled.set_index('D')\n",
    "    \n",
    "    # Resample to monthly data and calculate the sum for each month for the specified column\n",
    "    hydro_month = hydro_scaled[column].resample('M').sum().reset_index()\n",
    "    \n",
    "    # Extract year and month for grouping or further analysis\n",
    "    hydro_month['year'] = hydro_month['D'].dt.year\n",
    "    hydro_month['month'] = hydro_month['D'].dt.month\n",
    "    \n",
    "    return hydro_month\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b19d5b-0bd5-4942-b9ed-6957e937f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "\n",
    "column = 'PET'  # Set the target column\n",
    "location = 'mustang'\n",
    "\n",
    "folder_langtang = '/12a/'\n",
    "folder_mustang = '/13a/'\n",
    "\n",
    "folder_location = folder_langtang\n",
    "\n",
    "landcover1_folder = f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_daily/1landcover_40percent/{location}_climate_cut'\n",
    "landcover2_folder = f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_daily/1landcover_40percent/{location}_climate_cut'\n",
    "landcover3_folder = f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_daily/1landcover_40percent/{location}_climate_cut'\n",
    "landcover4_folder = f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_daily/1landcover_40percent/{location}_climate_cut'\n",
    "landcover5_folder = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/5landcover/{location}_climate_cut'\n",
    "\n",
    "\n",
    "output_path = '/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/hydro'\n",
    "\n",
    "# Define the list of folder paths\n",
    "folder_paths = [f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_daily/1landcover_40percent/{location}_climate_cut',\n",
    "                f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_daily/2landcover_40percent/{location}_climate_cut', \n",
    "                f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_daily/3landcover_40percent/{location}_climate_cut', \n",
    "                f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_daily/4landcover_40percent/{location}_climate_cut', \n",
    "                f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/5landcover/{location}_climate_cut']\n",
    "\n",
    "\n",
    "\n",
    "# Load elevation data and define the output path\n",
    "elevation = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/downscaling_simple/coordinates_and_elevation_with_labels_{location}.csv')[['cellnr2', 'band_data']]\n",
    "elevation = elevation.transpose()\n",
    "elevation_list = elevation.loc['cellnr2'].tolist()  # Adjust if you need a specific list format\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c589ef-14f0-423a-b336-f1d68055407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "\n",
    "# Iterate over folder paths\n",
    "for folder_path in folder_paths:\n",
    "    print(f\"Processing folder: {folder_path}\")\n",
    "\n",
    "    output_folder = output_path\n",
    "\n",
    "    # Extract \"1landcover\" from the folder path\n",
    "    landcover_part = os.path.basename(os.path.dirname(folder_path))\n",
    "    landcover_base = landcover_part.split('_')[0]  # This gives \"1landcover\"\n",
    "\n",
    "    # Load timestep DataFrame (assuming Hydro.out exists in the `/13a/` subfolder for all folders)\n",
    "    hydro_path = os.path.join(folder_path, '12a', 'Hydro.out')\n",
    "    print(f\"Checking Hydro.out path: {hydro_path}\")\n",
    "\n",
    "    if not os.path.isfile(hydro_path):\n",
    "        print(f\"Hydro.out not found in {hydro_path}\")\n",
    "        continue  # Skip this folder_path if the main Hydro.out is missing\n",
    "\n",
    "    # Load the Hydro.out file for timestep calculation\n",
    "    try:\n",
    "        dfts = pd.read_csv(hydro_path)\n",
    "        df = calculate_monthly_water_volume_all(dfts, column, area=4830.0)\n",
    "        df = df[['month', 'year']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading timestep file: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Initialize the result DataFrame for this folder_path\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over subfolders in the folder path\n",
    "    for folder_name in os.listdir(folder_path):\n",
    "        subfolder_path = os.path.join(folder_path, folder_name)\n",
    "        \n",
    "        # Skip files and hidden folders\n",
    "        if not os.path.isdir(subfolder_path) or folder_name.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(subfolder_path, 'Hydro.out')\n",
    "        print(f\"Checking subfolder Hydro.out: {file_path}\")\n",
    "\n",
    "        # Ensure the Hydro.out file exists in the subfolder\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                hydro = pd.read_csv(file_path)\n",
    "                output_df = calculate_monthly_water_volume_all(hydro, column)\n",
    "\n",
    "                # Extract and rename the target column\n",
    "                name_column = output_df[column]\n",
    "                column_name = f'{column}_{folder_name}'\n",
    "                result_df[column_name] = name_column\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Ensure result_df has data before proceeding\n",
    "    if result_df.empty:\n",
    "        print(f\"No valid data found in subfolders of {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    # Rename columns for clarity and filter by elevation list\n",
    "    result_df.columns = [col[-3:] for col in result_df.columns]  # Adjust column names if necessary\n",
    "    result_df = result_df[elevation_list]\n",
    "\n",
    "    # Check for consistency with the elevation list\n",
    "    if elevation_list == result_df.columns.tolist():\n",
    "        print(\"Default land cover. Monthly data: same\")\n",
    "\n",
    "        # Rename columns, merge with timestep, and add folder info\n",
    "        result_df.columns = elevation.loc['band_data']\n",
    "        result_df = pd.concat([df, result_df], axis=1)\n",
    "        result_df['folder'] = os.path.basename(folder_path)\n",
    "\n",
    "        print(result_df)\n",
    "\n",
    "        # Define the output file name and save the DataFrame to a CSV file\n",
    "        output_filename = f'{location}_monthly_sum_elevation_{column}_{landcover_base}.csv'\n",
    "        print('output filename:', output_filename)\n",
    "        result_df.to_csv(os.path.join(output_folder, output_filename), index=False)\n",
    "\n",
    "        print(f\"Saved {output_filename}\")\n",
    "    else:\n",
    "        print(f\"The data does not match the elevation list for {folder_path}!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf07f4-8850-4ecd-b5b6-e468a2a916d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = pd.read_csv('/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/5landcover/langtang_climate_cut/12a/Hydro.out')\n",
    "testfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad31d60-097b-4a15-91b0-599315dfc66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testfile.snow.plot()\n",
    "\n",
    "testfile.Pr.plot()\n",
    "testfile.snowacc.plot()\n",
    "\n",
    "testfile['snowmelt'] = testfile['snowacc'].where(testfile['snowacc'] < 0).fillna(0)\n",
    "testfile['snowacc2'] = testfile['snowacc'].where(testfile['snowacc'] > 0).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d922e-cad3-49e8-b917-b509a15ca8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile.snowacc2.plot()\n",
    "testfile.snowmelt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4912c4-2660-4e55-b37a-230265eaae9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc9dce-6394-4334-921e-e6a33e877a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7433538-10b0-41f0-b65a-8bc0eaac105f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c37bcc-5ce0-4e01-bded-f3f6a96ee271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78569ff-0c8e-4191-acd8-7d7fc3c7fa68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d655f7c6-bc56-4568-8e41-5a771d7da9fa",
   "metadata": {},
   "source": [
    "# calculate the monthly values in mm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7704a-a216-4e5e-af68-f8804ed81526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def water_balance_components_in_mm(hydro, column, area=4830.0):\n",
    "    \"\"\"\n",
    "    Calculate the sum of water or sediment volume per month for a specific column, \n",
    "    and return the mean volumes across years.\n",
    "    \n",
    "    Parameters:\n",
    "    hydro (pd.DataFrame): Input DataFrame with columns `D` (date) and the specified numeric column.\n",
    "    column (str): The name of the column to calculate the volume for.\n",
    "    area (float): Area in square meters used to scale the volumes. Default is 4830.0.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with the calculated monthly volumes.\n",
    "    \"\"\"\n",
    "    # Ensure `D` column is in datetime format\n",
    "    hydro['D'] = pd.to_datetime(hydro['D'])\n",
    "\n",
    "    if column == 'snowmelt':\n",
    "        hydro['snowmelt'] = hydro['snowacc'].where(hydro['snowacc'] < 0).fillna(0)\n",
    "    if column == 'snowacc2':\n",
    "        hydro['snowacc2'] = hydro['snowacc'].where(hydro['snowacc'] > 0).fillna(0)\n",
    "    if column == 'rainfall':\n",
    "        hydro['rainfall'] = hydro['Pr'] - hydro['snowacc']\n",
    "        hydro['rainfall'] = hydro['rainfall'].fillna(0)\n",
    "        \n",
    "    \n",
    "    # Scale the specified column by the area\n",
    "    hydro[column] = hydro[[column]]\n",
    "    \n",
    "    # Set 'D' as the index\n",
    "    hydro = hydro.set_index('D')\n",
    "    \n",
    "    # Resample to monthly data and calculate the sum for each month for the specified column\n",
    "    hydro_month = hydro[column].resample('M').sum().reset_index()\n",
    "    \n",
    "    # Extract year and month for grouping or further analysis\n",
    "    hydro_month['year'] = hydro_month['D'].dt.year\n",
    "    hydro_month['month'] = hydro_month['D'].dt.month\n",
    "    \n",
    "    return hydro_month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459bafd-9870-4cca-b470-b51843de4ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders = '/Volumes/6382452/!Feb2024/paper2/model_runs/TL_data/1landcover/langtang_climate_cut'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78e8e1-5555-4843-b92a-37a276188513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bd067-aa2f-45ce-8af9-5b4f66bdeaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# column = 'Pr'  # Set the target column\n",
    "location = 'mustang'\n",
    "\n",
    "folder_langtang = '/12a/'\n",
    "folder_mustang = '/13a/'\n",
    "\n",
    "\n",
    "# landcover1_folder = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/1landcover/{location}_climate_cut'\n",
    "# landcover2_folder = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/2landcover/{location}_climate_cut'\n",
    "# landcover3_folder = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/1landcover/{location}_climate_cut'\n",
    "# landcover4_folder = f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_daily/1landcover_40percent/{location}_climate_cut' #this one is different - no change in glacier \n",
    "# landcover5_folder = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/5landcover/{location}_climate_cut'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_path = '/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/hydro'\n",
    "\n",
    "# Define the list of folder paths\n",
    "# folder_paths = [f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_daily/1landcover_40percent/{location}_climate_cut',\n",
    "#                 f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_daily/2landcover_40percent/{location}_climate_cut', \n",
    "#                 f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_daily/3landcover_40percent/{location}_climate_cut', \n",
    "#                 f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_daily/4landcover_40percent/{location}_climate_cut', \n",
    "#                 f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/5landcover/{location}_climate_cut'     ]\n",
    "\n",
    "\n",
    "\n",
    "folder_paths = [f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/1landcover/{location}_climate_cut',\n",
    "                f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/2landcover/{location}_climate_cut',\n",
    "                f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/3landcover/{location}_climate_cut',\n",
    "                f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/4landcover/{location}_climate_cut',\n",
    "                f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/5landcover/{location}_climate_cut']\n",
    "\n",
    "# folder_paths = [f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/3landcover/{location}_climate_cut']\n",
    "\n",
    "\n",
    "\n",
    "# Load elevation data and define the output path\n",
    "elevation = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/downscaling_simple/coordinates_and_elevation_with_labels_{location}.csv')[['cellnr2', 'band_data']]\n",
    "elevation = elevation.transpose()\n",
    "elevation_list = elevation.loc['cellnr2'].tolist()  # Adjust if you need a specific list format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f162bb7b-2a6d-43e4-a9cc-289361426edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%%time\n",
    "\n",
    "print(column)\n",
    "\n",
    "# Iterate over folder paths\n",
    "for folder_path in folder_paths:\n",
    "    print(f\"Processing folder: {folder_path}\")\n",
    "\n",
    "    output_folder = output_path\n",
    "\n",
    "    # Extract \"1landcover\" from the folder path\n",
    "    landcover_part = os.path.basename(os.path.dirname(folder_path))\n",
    "    landcover_base = landcover_part.split('_')[0]  # This gives \"1landcover\"\n",
    "\n",
    "    # Load timestep DataFrame (assuming Hydro.out exists in the `/13a/` subfolder for all folders)\n",
    "    hydro_path = os.path.join(folder_path, '12a', 'Hydro.out')\n",
    "    print(f\"Checking Hydro.out path: {hydro_path}\")\n",
    "\n",
    "    if not os.path.isfile(hydro_path):\n",
    "        print(f\"Hydro.out not found in {hydro_path}\")\n",
    "        continue  # Skip this folder_path if the main Hydro.out is missing\n",
    "\n",
    "    # Load the Hydro.out file for timestep calculation\n",
    "    try:\n",
    "        dfts = pd.read_csv(hydro_path)\n",
    "        df = water_balance_components_in_mm(dfts, column, area=4830.0)\n",
    "        df = df[['month', 'year']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading timestep file: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Initialize the result DataFrame for this folder_path\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over subfolders in the folder path\n",
    "    for folder_name in os.listdir(folder_path):\n",
    "        subfolder_path = os.path.join(folder_path, folder_name)\n",
    "        \n",
    "        # Skip files and hidden folders\n",
    "        if not os.path.isdir(subfolder_path) or folder_name.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(subfolder_path, 'Hydro.out')\n",
    "        # print(f\"Checking subfolder Hydro.out: {file_path}\")\n",
    "\n",
    "        # Ensure the Hydro.out file exists in the subfolder\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                hydro = pd.read_csv(file_path)\n",
    "                output_df = water_balance_components_in_mm(hydro, column)\n",
    "\n",
    "                # Extract and rename the target column\n",
    "                name_column = output_df[column]\n",
    "                column_name = f'{column}_{folder_name}'\n",
    "                result_df[column_name] = name_column\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Ensure result_df has data before proceeding\n",
    "    if result_df.empty:\n",
    "        print(f\"No valid data found in subfolders of {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    # Rename columns for clarity and filter by elevation list\n",
    "    result_df.columns = [col[-3:] for col in result_df.columns]  # Adjust column names if necessary\n",
    "    result_df = result_df[elevation_list]\n",
    "\n",
    "    # Check for consistency with the elevation list\n",
    "    if elevation_list == result_df.columns.tolist():\n",
    "        print(\"Default land cover. Monthly data: same\")\n",
    "\n",
    "        # Rename columns, merge with timestep, and add folder info\n",
    "        result_df.columns = elevation.loc['band_data']\n",
    "        result_df = pd.concat([df, result_df], axis=1)\n",
    "        result_df['folder'] = os.path.basename(folder_path)\n",
    "\n",
    "        print(result_df)\n",
    "\n",
    "        # Define the output file name and save the DataFrame to a CSV file\n",
    "        output_filename = f'{location}_monthly_sum_elevation_{column}_{landcover_base}_mm.csv'\n",
    "        print('output filename:', output_filename)\n",
    "        result_df.to_csv(os.path.join(output_folder, output_filename), index=False)\n",
    "\n",
    "        print(f\"Saved {output_filename}\")\n",
    "    else:\n",
    "        print(f\"The data does not match the elevation list for {folder_path}!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8593a0bc-72ee-4cc4-8579-f5f8145b498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over columns too \n",
    "columns = ['PET', 'AET', 'Q', 'snowacc2', 'snowmelt', 'glacier_melt', 'Pr', 'rainfall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a06f5-4fa0-4381-8b5b-1da7dca0448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columns)  # Ensure `columns` is defined as a list of column names\n",
    "\n",
    "# Iterate over columns\n",
    "for column in columns:\n",
    "    print(f\"Processing column: {column}\")\n",
    "\n",
    "    # Iterate over folder paths\n",
    "    for folder_path in folder_paths:\n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "\n",
    "        output_folder = output_path\n",
    "\n",
    "        # Extract \"1landcover\" from the folder path\n",
    "        landcover_part = os.path.basename(os.path.dirname(folder_path))\n",
    "        landcover_base = landcover_part.split('_')[0]  # This gives \"1landcover\"\n",
    "\n",
    "        # Load timestep DataFrame (assuming Hydro.out exists in the `/12a/` subfolder for all folders)\n",
    "        hydro_path = os.path.join(folder_path, '13a', 'Hydro.out')\n",
    "        print(f\"Checking Hydro.out path: {hydro_path}\")\n",
    "\n",
    "        if not os.path.isfile(hydro_path):\n",
    "            print(f\"Hydro.out not found in {hydro_path}\")\n",
    "            continue  # Skip this folder_path if the main Hydro.out is missing\n",
    "\n",
    "        # Load the Hydro.out file for timestep calculation\n",
    "        try:\n",
    "            dfts = pd.read_csv(hydro_path)\n",
    "            df = water_balance_components_in_mm(dfts, column, area=4830.0)\n",
    "            df = df[['month', 'year']]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading timestep file: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize the result DataFrame for this folder_path\n",
    "        result_df = pd.DataFrame()\n",
    "\n",
    "        # Iterate over subfolders in the folder path\n",
    "        for folder_name in os.listdir(folder_path):\n",
    "            subfolder_path = os.path.join(folder_path, folder_name)\n",
    "            \n",
    "            # Skip files and hidden folders\n",
    "            if not os.path.isdir(subfolder_path) or folder_name.startswith('.'):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(subfolder_path, 'Hydro.out')\n",
    "            # print(f\"Checking subfolder Hydro.out: {file_path}\")\n",
    "\n",
    "            # Ensure the Hydro.out file exists in the subfolder\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    hydro = pd.read_csv(file_path)\n",
    "                    output_df = water_balance_components_in_mm(hydro, column)\n",
    "\n",
    "                    # Extract and rename the target column\n",
    "                    name_column = output_df[column]\n",
    "                    column_name = f'{column}_{folder_name}'\n",
    "                    result_df[column_name] = name_column\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Ensure result_df has data before proceeding\n",
    "        if result_df.empty:\n",
    "            print(f\"No valid data found in subfolders of {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        # Rename columns for clarity and filter by elevation list\n",
    "        result_df.columns = [col[-3:] for col in result_df.columns]  # Adjust column names if necessary\n",
    "        result_df = result_df[elevation_list]\n",
    "\n",
    "        # Check for consistency with the elevation list\n",
    "        if elevation_list == result_df.columns.tolist():\n",
    "            print(\"Default land cover. Monthly data: same\")\n",
    "\n",
    "            # Rename columns, merge with timestep, and add folder info\n",
    "            result_df.columns = elevation.loc['band_data']\n",
    "            result_df = pd.concat([df, result_df], axis=1)\n",
    "            result_df['folder'] = os.path.basename(folder_path)\n",
    "\n",
    "            print(result_df)\n",
    "\n",
    "            # Define the output file name and save the DataFrame to a CSV file\n",
    "            output_filename = f'{location}_monthly_sum_elevation_{column}_{landcover_base}_mm.csv'\n",
    "            print('output filename:', output_filename)\n",
    "            result_df.to_csv(os.path.join(output_folder, output_filename), index=False)\n",
    "\n",
    "            print(f\"Saved {output_filename}\")\n",
    "        else:\n",
    "            print(f\"The data does not match the elevation list for {folder_path}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9a824-7649-4440-8e16-c29021fc21dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ce3f87-742a-4941-aabf-e2c0b21035e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46269390-a84e-42c2-8119-73cb6a575666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f6a9e-dfdb-4f00-9ab8-87c5a91273c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "892a8adb-37df-4a93-b3e6-24daf32b0520",
   "metadata": {},
   "source": [
    "# plots! - water balance components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2852c-3572-4f9a-b6de-87b537daefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of interest: \n",
    "# barplots with + and - components \n",
    "# components:\n",
    "# AET PET Q GLACIER_MELT SNOW_MELT SNOW_ACC \n",
    "\n",
    "# + Pr, snow melt, glacier melt\n",
    "# - AET PET Q \n",
    "\n",
    "\n",
    "def prepare_for_boxplots(df, value_name):\n",
    "    # value name - column in the output dataframe\n",
    "    melted = pd.melt(df, id_vars=['month','folder', 'year'], var_name='elevation', value_name=value_name)\n",
    "    melted['elevation'] = melted['elevation'].str.split('.').str[0].astype(float)\n",
    "    melted['elevation_bin'] = melted.apply(functions.bin_elevation500, axis=1)\n",
    "    melted = melted.sort_values('elevation_bin')\n",
    "    return melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82361ba-897a-4efd-a8ee-beecfd52f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for landcover 1\n",
    "landcover_idx = 3\n",
    "location = 'langtang' # 'mustang'\n",
    "aet1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/hydro/{location}_monthly_sum_elevation_AET_{landcover_idx}landcover_mm.csv')\n",
    "pet1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/hydro/{location}_monthly_sum_elevation_PET_{landcover_idx}landcover_mm.csv')\n",
    "Q1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/hydro/{location}_monthly_sum_elevation_Q_{landcover_idx}landcover_mm.csv')\n",
    "glmelt1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/hydro/{location}_monthly_sum_elevation_glacier_melt_{landcover_idx}landcover_mm.csv')\n",
    "snowmelt1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/hydro/{location}_monthly_sum_elevation_snowmelt_{landcover_idx}landcover_mm.csv')\n",
    "pr1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/hydro/{location}_monthly_sum_elevation_Pr_{landcover_idx}landcover_mm.csv')\n",
    "snowacc1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/hydro/{location}_monthly_sum_elevation_snowacc2_{landcover_idx}landcover_mm.csv')\n",
    "rainfall1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/hydro/{location}_monthly_sum_elevation_rainfall_{landcover_idx}landcover_mm.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685dd62-c6cb-4609-bb70-262805bb3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "aet1_melted = prepare_for_boxplots(aet1, 'AET')\n",
    "pet1_melted = prepare_for_boxplots(pet1, 'PET')\n",
    "Q1_melted = prepare_for_boxplots(Q1, 'Q')\n",
    "\n",
    "glmelt1_melted = prepare_for_boxplots(glmelt1, 'glmelt')\n",
    "snowmelt1_melted = prepare_for_boxplots(snowmelt1, 'snowmelt')\n",
    "pr1_melted = prepare_for_boxplots(pr1, 'Pr')\n",
    "snowacc1_melted = prepare_for_boxplots(snowacc1, 'snowacc')\n",
    "\n",
    "\n",
    "# make snowmelt not negative \n",
    "snowmelt1_melted['snowmelt'] = snowmelt1_melted['snowmelt'] * -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a9862-9b06-4cf3-88a4-7c00c8fe4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20), layout = 'tight')\n",
    "mosaic = fig.subplot_mosaic('''\n",
    "                            aaa\n",
    "                            bbb\n",
    "                            ccc\n",
    "                            eee\n",
    "                            fff\n",
    "                            ggg\n",
    "                            ''')\n",
    "\n",
    "\n",
    "sns.barplot(ax = mosaic['a'], x='month', y='Pr', hue = 'elevation_bin', data=pr1_melted, palette = 'magma_r', legend = True) \n",
    "sns.barplot(ax = mosaic['b'], x='month', y='AET', hue = 'elevation_bin', data=aet1_melted, palette = 'viridis_r', legend = False) \n",
    "sns.barplot(ax = mosaic['c'], x='month', y='PET', hue = 'elevation_bin', data=pet1_melted, palette = 'viridis_r', legend = True)\n",
    "# sns.barplot(ax = mosaic['d'], x='month', y='snowacc', hue = 'elevation_bin', data=snowacc1_melted, palette = 'magma_r', legend = False)\n",
    "sns.barplot(ax = mosaic['e'], x='month', y='snowmelt', hue = 'elevation_bin', data=snowmelt1_melted, palette = 'magma_r', legend = False)\n",
    "sns.barplot(ax = mosaic['f'], x='month', y='glmelt', hue = 'elevation_bin', data=glmelt1_melted, palette = 'magma_r', legend = False)\n",
    "sns.barplot(ax = mosaic['g'], x='month', y='Q', hue = 'elevation_bin', data=Q1_melted, palette = 'magma_r', legend = False)\n",
    "\n",
    "\n",
    "sns.move_legend(mosaic['a'], \"upper left\", bbox_to_anchor=(1, 1))\n",
    "sns.move_legend(mosaic['c'], \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "# # Adding text to the top-left corner of each subplot\n",
    "# mosaic['a'].text(0.02, 0.95, 'veg 40%, bedrock 40%, ice 20%', transform=mosaic['a'].transAxes, fontsize=25, verticalalignment='top')\n",
    "# mosaic['b'].text(0.02, 0.95, 'veg 40%, bedrock 50%, ice 10%', transform=mosaic['b'].transAxes, fontsize=25, verticalalignment='top')\n",
    "# mosaic['c'].text(0.02, 0.95, 'veg 50%, bedrock 40%, ice 10%', transform=mosaic['c'].transAxes, fontsize=25, verticalalignment='top')\n",
    "# mosaic['d'].text(0.02, 0.95, 'veg 40%, bedrock 60%, ice 0%',  transform=mosaic['d'].transAxes, fontsize=25, verticalalignment='top')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if landcover_idx ==1:\n",
    "    plt.suptitle('veg 40%, bedrock 40%, ice 20%', fontsize = 25)\n",
    "if landcover_idx ==2:\n",
    "    plt.suptitle('veg 40%, bedrock 50%, ice 10%', fontsize = 25)\n",
    "if landcover_idx ==3:\n",
    "    plt.suptitle('veg 50%, bedrock 40%, ice 10%', fontsize = 25)\n",
    "\n",
    "# plt.savefig(f'water_balance_components_landcover{landcover_idx}.png', dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea4582-be17-4db9-855b-786ad576b05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab92d4-e4b0-441d-8e73-a18367cfbae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8fb42b-9de2-47b1-9417-85b4d5340691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a77981-51bc-4346-8a40-033c878fc697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc3dd9-d35c-49e9-9635-95ad15291e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6db14-5b98-42ea-8cf8-71504d07d829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e03fe-a93c-4692-a741-9a50934023ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b06ec5-c2d1-4de5-b4ff-8ef96d40b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "glmelt1 = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/hydro/langtang_monthly_sum_elevation_glacier_melt_{landcover_idx}landcover_mm.csv')\n",
    "glmelt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ebfed-e62d-4bf2-84cb-cb1cc5896746",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20), layout = 'tight')\n",
    "\n",
    "glmelt1.set_index(['year', 'month']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297064b3-964d-4a67-9e45-35a3de84bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = pd.read_csv('/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/TL_data/1landcover/langtang_climate_cut/12a/Hydro.out')\n",
    "testfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6909708-c351-4cb3-913a-170418f8b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5), layout = 'tight')\n",
    "testfile[1:10000].glacier_melt.plot(label = 'glacier')\n",
    "testfile[1:10000].Pr.plot(label = 'Pr')\n",
    "testfile[1:10000].Ta.plot(label = 'Ta')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe3dfe5-1dad-401d-ac00-a54c92ebbe05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f83515-75c6-422d-a87c-f888b2722499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sedcas] *",
   "language": "python",
   "name": "conda-env-sedcas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
