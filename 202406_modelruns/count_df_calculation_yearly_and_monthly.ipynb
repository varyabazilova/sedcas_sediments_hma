{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6843ee3a-e51c-4702-ad3c-cec9d8c7911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "\n",
    "import functions\n",
    "\n",
    "\n",
    "# the data is stored on the external ssd \n",
    "# the output should go to the local folder \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de4632-d651-4ba7-8231-400140efb0af",
   "metadata": {},
   "source": [
    "# per year\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73fab8-af2a-4f6c-a47e-3afd6dc5cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '/Volumes/T7 Shield/202409_paper2_modelruns/30years/SL_daily/1landcover_20percent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18af281-f636-4184-9edb-f98eb4af9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder path - data that sits on the SSD drive - outout of model runs \n",
    "# output_path - folder to save all the things, related to the scenario \n",
    "# dfspot - potential dfs = TL situation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3658e47e-cf62-4e6e-9410-07244fa70853",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "\n",
    "# List of landcover indices and percentiles\n",
    "# landcover_indices = [1, 2, 3, 4, 5]\n",
    "landcover_indices = [3, 5]#, 3, 4, 5]\n",
    "\n",
    "percentiles = ['20percent']#, '30percent', '40percent', '50percent', '60percent']\n",
    "# percentiles = ['50percent', '60percent']\n",
    "location = 'mustang'\n",
    "\n",
    "method = 'daily'\n",
    "column = 'dfspot'  \n",
    "freq = 'year'  \n",
    "\n",
    "# Load elevation data once, outside the loops\n",
    "elevation = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/downscaling_simple/coordinates_and_elevation_with_labels_{location}.csv')[['cellnr2', 'band_data']]\n",
    "\n",
    "# Iterate over each percentile\n",
    "for percentile in percentiles:\n",
    "    # Define the output path for the current percentile\n",
    "    # output_path = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025Jan_output/SL_{method}/output_{percentile}'\n",
    "    output_path = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025Jan_output/TL/test'\n",
    "    \n",
    "    # Iterate over each landcover index\n",
    "    for landcover_idx in landcover_indices:\n",
    "        # folder_path = f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_{method}/{landcover_idx}landcover_{percentile}/{location}_climate_cut'\n",
    "        folder_path = f'/Volumes/T7 Shield/202409_paper2_modelruns/30years/SL_{method}/{landcover_idx}landcover_{percentile}/{location}_climate_cut'\n",
    "        result_df = pd.DataFrame()\n",
    "\n",
    "        # Iterate over the files in the folders\n",
    "        for folder_name in os.listdir(folder_path):\n",
    "            # Skip all csvs and hidden files\n",
    "            if folder_name.endswith('.csv') or folder_name.startswith('.'):\n",
    "                continue\n",
    "            \n",
    "            file_path = os.path.join(folder_path, folder_name, 'Sediment.out')\n",
    "            \n",
    "            if os.path.isfile(file_path):\n",
    "                # Read file\n",
    "                output_df = pd.read_csv(file_path)\n",
    "                # Count dfs per given time\n",
    "                output_df = functions.count_dfs_per_time(output_df, column, freq)\n",
    "                \n",
    "                # Take the COUNT column\n",
    "                name_column = output_df['count']\n",
    "                # Rename the columns\n",
    "                column_name = f'{column}_{folder_name}'\n",
    "                result_df[column_name] = name_column\n",
    "                result_df.columns = [col[-3:] for col in result_df.columns]\n",
    "\n",
    "\n",
    "        # Add elevation data to the result_df\n",
    "        annaul_df_count = functions.add_elevation_to_df_count(result_df, elevation, freq)\n",
    "\n",
    "        # Save the resulting DataFrame to a CSV file for the current landcover index and percentile\n",
    "        output_filename = f'{location}_annual_{column}_count{percentile}_landcover{landcover_idx}.csv'\n",
    "        annaul_df_count.to_csv(os.path.join(output_path, output_filename))\n",
    "\n",
    "        print(f\"Processed {percentile} for landcover index {landcover_idx}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0909f0f-96c8-4750-871c-14958499b1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3281b8-5318-4a04-9eae-f04027c45a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ed8bf-d13a-4e84-a85e-1f00770d0728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc3c1b77-f8dc-4c70-ba73-3053613c076e",
   "metadata": {},
   "source": [
    "# month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "107775a4-a5ac-408e-983b-c0c3f150d201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025Jan_output/SL_once/output_20percent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default land cover. monthly data: same\n",
      "Processed 20percent for landcover index 1\n",
      "default land cover. monthly data: same\n",
      "Processed 20percent for landcover index 2\n",
      "default land cover. monthly data: same\n",
      "Processed 20percent for landcover index 3\n",
      "default land cover. monthly data: same\n",
      "Processed 20percent for landcover index 4\n",
      "default land cover. monthly data: same\n",
      "Processed 20percent for landcover index 5\n",
      "/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025Jan_output/SL_once/output_30percent\n",
      "default land cover. monthly data: same\n",
      "Processed 30percent for landcover index 1\n",
      "default land cover. monthly data: same\n",
      "Processed 30percent for landcover index 2\n",
      "default land cover. monthly data: same\n",
      "Processed 30percent for landcover index 3\n",
      "default land cover. monthly data: same\n",
      "Processed 30percent for landcover index 4\n",
      "default land cover. monthly data: same\n",
      "Processed 30percent for landcover index 5\n",
      "/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025Jan_output/SL_once/output_40percent\n",
      "default land cover. monthly data: same\n",
      "Processed 40percent for landcover index 1\n",
      "default land cover. monthly data: same\n",
      "Processed 40percent for landcover index 2\n",
      "default land cover. monthly data: same\n",
      "Processed 40percent for landcover index 3\n",
      "default land cover. monthly data: same\n",
      "Processed 40percent for landcover index 4\n",
      "default land cover. monthly data: same\n",
      "Processed 40percent for landcover index 5\n",
      "/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025Jan_output/SL_once/output_50percent\n",
      "default land cover. monthly data: same\n",
      "Processed 50percent for landcover index 1\n",
      "default land cover. monthly data: same\n",
      "Processed 50percent for landcover index 2\n",
      "default land cover. monthly data: same\n",
      "Processed 50percent for landcover index 3\n",
      "default land cover. monthly data: same\n",
      "Processed 50percent for landcover index 4\n",
      "default land cover. monthly data: same\n",
      "Processed 50percent for landcover index 5\n",
      "/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025Jan_output/SL_once/output_60percent\n",
      "default land cover. monthly data: same\n",
      "Processed 60percent for landcover index 1\n",
      "default land cover. monthly data: same\n",
      "Processed 60percent for landcover index 2\n",
      "default land cover. monthly data: same\n",
      "Processed 60percent for landcover index 3\n",
      "default land cover. monthly data: same\n",
      "Processed 60percent for landcover index 4\n",
      "default land cover. monthly data: same\n",
      "Processed 60percent for landcover index 5\n",
      "CPU times: user 27min 48s, sys: 54.5 s, total: 28min 42s\n",
      "Wall time: 29min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# List of landcover indices and percentiles\n",
    "landcover_indices = [1, 2, 3, 4, 5]\n",
    "percentiles = ['20percent', '30percent', '40percent', '50percent', '60percent']\n",
    "\n",
    "location = 'mustang'\n",
    "\n",
    "method = 'once'\n",
    "column = 'dfs'  \n",
    "freq = 'month'  \n",
    "\n",
    "# Load elevation data once, outside the loops\n",
    "elevation = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/downscaling_simple/coordinates_and_elevation_with_labels_{location}.csv')[['cellnr2', 'band_data']]\n",
    "\n",
    "# Iterate over each percentile\n",
    "for percentile in percentiles:\n",
    "    # Define the output path for the current percentile\n",
    "    output_path = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025Jan_output/SL_{method}/output_{percentile}'\n",
    "    print(output_path)\n",
    "    \n",
    "    # Iterate over each landcover index\n",
    "    for landcover_idx in landcover_indices:\n",
    "        folder_path = f'/Volumes/T7 Shield/202409_paper2_modelruns/30years/SL_{method}/{landcover_idx}landcover_{percentile}/{location}_climate_cut'\n",
    "\n",
    "        result_df = pd.DataFrame()\n",
    "\n",
    "        # Iterate over the files in the folders\n",
    "        for folder_name in os.listdir(folder_path):\n",
    "            # Skip all csvs and hidden files\n",
    "            if folder_name.endswith('.csv') or folder_name.startswith('.'):\n",
    "                continue\n",
    "            \n",
    "            file_path = os.path.join(folder_path, folder_name, 'Sediment.out')\n",
    "            \n",
    "            if os.path.isfile(file_path):\n",
    "                # Read file\n",
    "                output_df = pd.read_csv(file_path)\n",
    "                # Count dfs per given time\n",
    "                output_df = functions.count_dfs_per_time(output_df, column, freq)\n",
    "                \n",
    "                # Take the COUNT column\n",
    "                name_column = output_df['count']\n",
    "                # Rename the columns\n",
    "                column_name = f'{column}_{folder_name}'\n",
    "                result_df[column_name] = name_column\n",
    "                result_df.columns = [col[-3:] for col in result_df.columns]\n",
    "\n",
    "        # Add elevation data to the result_df\n",
    "        annaul_df_count = functions.add_elevation_to_df_count(result_df, elevation, freq)\n",
    "\n",
    "        # Save the resulting DataFrame to a CSV file for the current landcover index and percentile\n",
    "        output_filename = f'{location}_monthly_{column}_count_{percentile}_landcover{landcover_idx}.csv'\n",
    "        annaul_df_count.to_csv(os.path.join(output_path, output_filename))\n",
    "\n",
    "        print(f\"Processed {percentile} for landcover index {landcover_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ddd223-f64c-43a5-8873-837f3bc2fd13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287992db-692f-49c1-aafc-d1a3f72953da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b2c4f-46fa-4575-9825-281222c237db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd92a4-53b4-4f59-a971-f904a4fe9cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d73d7-b299-45b5-b36f-50509355917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dfs_per_time(sediments, column, freq):\n",
    "    '''\n",
    "    sediments - sediment output file\n",
    "    column - column of interest (e.g. dfs)\n",
    "    freq - resampling frequency\n",
    "    '''\n",
    "\n",
    "    sediments['D'] = pd.to_datetime(sediments.D)\n",
    "    sediments = sediments.set_index('D')\n",
    "    # sediments = sediments[sediments[column] > 0]\n",
    "    sediments['count'] = sediments[column].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    \n",
    "    if freq == 'month':\n",
    "        # calculate how many dfs are there per month \n",
    "        sym = sediments.resample('m').sum()\n",
    "        return sym\n",
    "    \n",
    "    elif freq == 'year':\n",
    "        # calculate how many dfs are there per year \n",
    "        sym_year = sediments.resample('Y').sum()\n",
    "        return sym_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657edd12-348f-42fd-9cd1-ded6a25926df",
   "metadata": {},
   "source": [
    "# TL (dfspot count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336281c5-734e-443d-8485-afda4b61230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default land cover. monthly data: same\n",
      "Processed 20percent for landcover index 1\n",
      "default land cover. monthly data: same\n",
      "Processed 20percent for landcover index 2\n",
      "default land cover. monthly data: same\n",
      "Processed 20percent for landcover index 3\n",
      "default land cover. monthly data: same\n",
      "Processed 20percent for landcover index 4\n",
      "default land cover. monthly data: same\n",
      "Processed 20percent for landcover index 5\n",
      "CPU times: user 5min 40s, sys: 13.8 s, total: 5min 53s\n",
      "Wall time: 6min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# List of landcover indices and percentiles\n",
    "landcover_indices = [1, 2, 3, 4, 5]\n",
    "percentiles = ['20percent']\n",
    "# landcover_indices = [3]\n",
    "\n",
    "\n",
    "location = 'mustang'\n",
    "\n",
    "# method = 'daily'\n",
    "column = 'dfspot'  \n",
    "freq = 'month'\n",
    "\n",
    "# Load elevation data once, outside the loops\n",
    "elevation = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/downscaling_simple/coordinates_and_elevation_with_labels_{location}.csv')[['cellnr2', 'band_data']]\n",
    "\n",
    "# Iterate over each percentile\n",
    "for percentile in percentiles:\n",
    "    # Define the output path for the current percentile\n",
    "    output_path = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025Jan_output/TL/march2025test2'\n",
    "    \n",
    "    # Iterate over each landcover index\n",
    "    for landcover_idx in landcover_indices:\n",
    "        # folder_path = f'/Volumes/T7 Shield/202409_paper2_modelruns/30years/SL_{method}/{landcover_idx}landcover_{percentile}/{location}_climate_cut'\n",
    "        folder_path = f'/Volumes/T7 Shield/202409_paper2_modelruns/30years/TL_data/{landcover_idx}landcover_{percentile}/{location}_climate_cut'\n",
    "        # folder_path = f'/Volumes/T7 Shield/202409_paper2_modelruns/30years/test/'\n",
    "        result_df = pd.DataFrame()\n",
    "\n",
    "        # Iterate over the files in the folders\n",
    "        for folder_name in os.listdir(folder_path):\n",
    "            # Skip all csvs and hidden files\n",
    "            if folder_name.endswith('.csv') or folder_name.startswith('.'):\n",
    "                continue\n",
    "            \n",
    "            file_path = os.path.join(folder_path, folder_name, 'Sediment.out')\n",
    "            \n",
    "            if os.path.isfile(file_path):\n",
    "                # Read file\n",
    "                output_df = pd.read_csv(file_path)\n",
    "                # Count dfs per given time\n",
    "                output_df = functions.count_dfs_per_time(output_df, column, freq)\n",
    "                \n",
    "                # Take the COUNT column\n",
    "                name_column = output_df['count']\n",
    "                # Rename the columns\n",
    "                column_name = f'{column}_{folder_name}'\n",
    "                result_df[column_name] = name_column\n",
    "                result_df.columns = [col[-3:] for col in result_df.columns]\n",
    "\n",
    "        # Add elevation data to the result_df\n",
    "        annaul_df_count = functions.add_elevation_to_df_count(result_df, elevation, freq)\n",
    "\n",
    "        # Save the resulting DataFrame to a CSV file for the current landcover index and percentile\n",
    "        output_filename = f'{location}_monthly_{column}_count_{percentile}_landcover{landcover_idx}.csv'\n",
    "        annaul_df_count.to_csv(os.path.join(output_path, output_filename))\n",
    "\n",
    "        print(f\"Processed {percentile} for landcover index {landcover_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e5c5a-8dbd-4613-912c-25429acc3071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a6c68-620e-4840-aad9-1db14d06d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe472b-4ef2-438f-ba5e-4b75f2441dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96d0a7-60df-4042-bdc4-5f819bf8edbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23aa664-e285-44bb-b720-057c1a720f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c5fb3-f4c4-4e6d-9ebb-7af1bc3cf237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225764d-2e39-4d6f-bbc2-87e0999d581e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af28727-7bfc-4f8e-9e2e-ae1ec2af5693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e3800e-913f-4105-87ac-6df8d67a1776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb125db-cf04-425c-ad4c-b19269c81d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592dfb44-b3bd-46cd-9506-1be59cfeebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/Volumes/T7 Shield/202409_paper2_modelruns/30years/TL_data/3landcover_20percent/mustang_climate_cut/34a/Sediment.out')\n",
    "\n",
    "test.dfspot.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63663652-36a5-4c6b-910b-bcd9303f1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dfs_per_time(sediments, column, freq):\n",
    "    '''\n",
    "    sediments - sediment output file\n",
    "    column - column of interest (e.g. dfs)\n",
    "    freq - resampling frequency\n",
    "    '''\n",
    "\n",
    "    sediments['D'] = pd.to_datetime(sediments.D)\n",
    "    sediments = sediments.set_index('D')\n",
    "    sediments = sediments[sediments[column] > 0]\n",
    "    sediments['count'] = sediments[column].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    \n",
    "    if freq == 'month':\n",
    "        # calculate how many dfs are there per month \n",
    "        sym = sediments.resample('m').sum()\n",
    "        return sym\n",
    "    \n",
    "    elif freq == 'year':\n",
    "        # calculate how many dfs are there per year \n",
    "        sym_year = sediments.resample('Y').sum()\n",
    "        return sym_year\n",
    "\n",
    "\n",
    "\n",
    "testdf = functions.count_dfs_per_time(test, 'dfspot', 'month')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66212aa-58ef-47b2-a970-a70f56468fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.dfspot.plot(figsize = (20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87424a3-5e19-42ee-9f9e-6882f6bb60e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61492615-509a-4cba-a3b1-bc5fa85fe4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5ad51-63d5-4c5f-8cc4-f9ceae29a0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe1f65-6af2-4cb0-a938-456e6021b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = testdf.reset_index()\n",
    "testdf['Dyear'] = testdf.D.dt.year\n",
    "\n",
    "testdf['Dmonth'] = testdf.D.dt.month\n",
    "\n",
    "testdf['dateid'] = testdf['Dyear'].astype(str) + \"_\" + testdf['Dmonth'].astype(str).str.zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e3e449-ef69-4cc2-92f1-458a5c00adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(testdf['index'], testdf['count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d1371c-fd3f-4c60-8750-ab9d12aba2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.count.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874b4dc-b9bc-4d46-a23c-b03b51d475f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f8ddf-51b2-4dac-a24a-4b6ed669542d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8b238-33e0-4081-bfcd-42ee3ad536a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8017e28-2959-46f4-869e-d2f46f547650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621ba51e-d898-45cd-9af5-910653fc311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# List of landcover indices and percentiles\n",
    "landcover_indices = [1, 2, 3, 4, 5]\n",
    "percentiles = ['40percent']\n",
    "\n",
    "location = 'mustang'\n",
    "\n",
    "method = 'daily'\n",
    "column = 'dfspot'  \n",
    "freq = 'month'  \n",
    "\n",
    "# Load elevation data once, outside the loops\n",
    "elevation = pd.read_csv(f'/Users/varyabazilova/Desktop/paper2/downscaling_simple/coordinates_and_elevation_with_labels_{location}.csv')[['cellnr2', 'band_data']]\n",
    "\n",
    "# Iterate over each percentile\n",
    "for percentile in percentiles:\n",
    "    # Define the output path for the current percentile\n",
    "    output_path = f'/Users/varyabazilova/Desktop/paper2/202406_modelruns/30years/2025Jan_output/TL/'\n",
    "    \n",
    "    # Iterate over each landcover index\n",
    "    for landcover_idx in landcover_indices:\n",
    "        folder_path = f'/Volumes/T7 Shield/202409_paper2_modelruns/30years/SL_{method}/{landcover_idx}landcover_{percentile}/{location}_climate_cut'\n",
    "        # folder_path = f'/Volumes/Extreme SSD/202409_paper2_modelruns/30years/SL_{method}/{landcover_idx}landcover_{percentile}/{location}_climate_cut'\n",
    "        result_df = pd.DataFrame()\n",
    "\n",
    "        # Iterate over the files in the folders\n",
    "        for folder_name in os.listdir(folder_path):\n",
    "            # Skip all csvs and hidden files\n",
    "            if folder_name.endswith('.csv') or folder_name.startswith('.'):\n",
    "                continue\n",
    "            \n",
    "            file_path = os.path.join(folder_path, folder_name, 'Sediment.out')\n",
    "            \n",
    "            if os.path.isfile(file_path):\n",
    "                # Read file\n",
    "                output_df = pd.read_csv(file_path)\n",
    "                # Count dfs per given time\n",
    "                output_df = functions.count_dfs_per_time(output_df, column, freq)\n",
    "                \n",
    "                # Take the COUNT column\n",
    "                name_column = output_df['count']\n",
    "                # Rename the columns\n",
    "                column_name = f'{column}_{folder_name}'\n",
    "                result_df[column_name] = name_column\n",
    "                result_df.columns = [col[-3:] for col in result_df.columns]\n",
    "\n",
    "        # Add elevation data to the result_df\n",
    "        annaul_df_count = functions.add_elevation_to_df_count(result_df, elevation, freq)\n",
    "\n",
    "        # Save the resulting DataFrame to a CSV file for the current landcover index and percentile\n",
    "        output_filename = f'{location}_monthly_{column}_count_landcover{landcover_idx}.csv'\n",
    "        annaul_df_count.to_csv(os.path.join(output_path, output_filename))\n",
    "\n",
    "        print(f\"Processed {percentile} for landcover index {landcover_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b18e5e-fb34-4c6e-9ed9-64c3c7617c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa6e58-ef98-4e14-bfa2-9749781e64b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122a515-3086-4d3c-b3b2-574fe7d29ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2f228-9a05-4e7a-94a3-491167fb5bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dbb35d-dc3c-4e0b-bee3-a1f94a69451a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc29d5d-f3f5-4d23-9934-6b14604b0c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc38799-2b5a-491f-a517-184771ffc598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60df880-9454-4ac8-97c5-63367f0067e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65b450-c93f-4ab6-b2c0-4fbe338e7f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad454207-6f78-46a4-bfc5-cd4bf5e86a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dfs_per_time(sediments, column, freq):\n",
    "    '''\n",
    "    sediments - sediment output file\n",
    "    column - column of interest (e.g. dfs)\n",
    "    freq - resampling frequency\n",
    "    '''\n",
    "\n",
    "    sediments['D'] = pd.to_datetime(sediments.D)\n",
    "    sediments = sediments.set_index('D')\n",
    "    sediments = sediments[sediments[column] > 0]\n",
    "    sediments['count'] = sediments[column].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    \n",
    "    if freq == 'month':\n",
    "        # calculate how many dfs are there per month \n",
    "        sym = sediments.resample('m').sum()\n",
    "        return sym\n",
    "    \n",
    "    elif freq == 'year':\n",
    "        # calculate how many dfs are there per year \n",
    "        sym_year = sediments.resample('Y').sum()\n",
    "        return sym_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e9317-bdfc-435c-896f-abba2da3445f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f5e34-ad42-4d89-8c51-a89fb9589c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576c173-7643-4a2b-87f6-c9a5f3e538a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39908a41-e0e5-4ee9-8278-f999b0fc99d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sedcas] *",
   "language": "python",
   "name": "conda-env-sedcas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
